{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87fe09ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7d450ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "# Generate a unique run ID for this experiment\n",
    "run_uid = uuid.uuid4().hex[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cdb4115",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_benchmarks import clone_public_dataset, registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1078447b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>Name                   </th><th>Type         </th><th>Dataset ID                                                                                                                                                 </th><th>Description  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>LangChain Docs Q&A     </td><td>RetrievalTask</td><td><a href=\"https://smith.langchain.com/public/452ccafc-18e1-4314-885b-edd735f17b9d/d\" target=\"_blank\" rel=\"noopener\">452ccafc-18e1-4314-885b-edd735f17b9d</a></td><td>Questions and answers based on a snapshot of the LangChain python docs.\n",
       "\n",
       "The environment provides the documents and the retriever information.\n",
       "\n",
       "Each example is composed of a question and reference answer.\n",
       "\n",
       "Success is measured based on the accuracy of the answer relative to the reference answer.\n",
       "We also measure the faithfulness of the model's response relative to the retrieved documents (if any).              </td></tr>\n",
       "<tr><td>Semi-structured Reports</td><td>RetrievalTask</td><td><a href=\"https://smith.langchain.com/public/c47d9617-ab99-4d6e-a6e6-92b8daf85a7d/d\" target=\"_blank\" rel=\"noopener\">c47d9617-ab99-4d6e-a6e6-92b8daf85a7d</a></td><td>Questions and answers based on PDFs containing tables and charts.\n",
       "\n",
       "The task provides the raw documents as well as factory methods to easily index them\n",
       "and create a retriever.\n",
       "\n",
       "Each example is composed of a question and reference answer.\n",
       "\n",
       "Success is measured based on the accuracy of the answer relative to the reference answer.\n",
       "We also measure the faithfulness of the model's response relative to the retrieved documents (if any).              </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "Registry(tasks=[RetrievalTask(name='LangChain Docs Q&A', dataset_id='https://smith.langchain.com/public/452ccafc-18e1-4314-885b-edd735f17b9d/d', description=\"Questions and answers based on a snapshot of the LangChain python docs.\\n\\nThe environment provides the documents and the retriever information.\\n\\nEach example is composed of a question and reference answer.\\n\\nSuccess is measured based on the accuracy of the answer relative to the reference answer.\\nWe also measure the faithfulness of the model's response relative to the retrieved documents (if any).\\n\", retriever_factories={'basic': <function _chroma_retriever_factory at 0x13f2a7820>, 'parent-doc': <function _chroma_parent_document_retriever_factory at 0x13f2a78b0>, 'hyde': <function _chroma_hyde_retriever_factory at 0x13f2a7940>}, architecture_factories={'conversational-retrieval-qa': <function default_response_chain at 0x139a454c0>}, get_docs=<function load_cached_docs at 0x139a45310>), RetrievalTask(name='Semi-structured Reports', dataset_id='https://smith.langchain.com/public/c47d9617-ab99-4d6e-a6e6-92b8daf85a7d/d', description=\"Questions and answers based on PDFs containing tables and charts.\\n\\nThe task provides the raw documents as well as factory methods to easily index them\\nand create a retriever.\\n\\nEach example is composed of a question and reference answer.\\n\\nSuccess is measured based on the accuracy of the answer relative to the reference answer.\\nWe also measure the faithfulness of the model's response relative to the retrieved documents (if any).\\n\", retriever_factories={'basic': <function _chroma_retriever_factory at 0x13f2a7dc0>, 'parent-doc': <function _chroma_parent_document_retriever_factory at 0x13f2a7e50>, 'hyde': <function _chroma_hyde_retriever_factory at 0x13f2a7ee0>}, architecture_factories={}, get_docs=<function load_docs at 0x13f2a7d30>)])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "registry = registry.filter(Type=\"RetrievalTask\")\n",
    "registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bdae45a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody>\n",
       "<tr><td>Name                  </td><td>LangChain Docs Q&A                                                                                                                                         </td></tr>\n",
       "<tr><td>Type                  </td><td>RetrievalTask                                                                                                                                              </td></tr>\n",
       "<tr><td>Dataset ID            </td><td><a href=\"https://smith.langchain.com/public/452ccafc-18e1-4314-885b-edd735f17b9d/d\" target=\"_blank\" rel=\"noopener\">452ccafc-18e1-4314-885b-edd735f17b9d</a></td></tr>\n",
       "<tr><td>Description           </td><td>Questions and answers based on a snapshot of the LangChain python docs.\n",
       "\n",
       "The environment provides the documents and the retriever information.\n",
       "\n",
       "Each example is composed of a question and reference answer.\n",
       "\n",
       "Success is measured based on the accuracy of the answer relative to the reference answer.\n",
       "We also measure the faithfulness of the model's response relative to the retrieved documents (if any).                                                                                                                                                            </td></tr>\n",
       "<tr><td>Retriever Factories   </td><td>basic, parent-doc, hyde                                                                                                                                    </td></tr>\n",
       "<tr><td>Architecture Factories</td><td>conversational-retrieval-qa                                                                                                                                </td></tr>\n",
       "<tr><td>get_docs              </td><td><function load_cached_docs at 0x139a45310>                                                                                                                 </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "RetrievalTask(name='LangChain Docs Q&A', dataset_id='https://smith.langchain.com/public/452ccafc-18e1-4314-885b-edd735f17b9d/d', description=\"Questions and answers based on a snapshot of the LangChain python docs.\\n\\nThe environment provides the documents and the retriever information.\\n\\nEach example is composed of a question and reference answer.\\n\\nSuccess is measured based on the accuracy of the answer relative to the reference answer.\\nWe also measure the faithfulness of the model's response relative to the retrieved documents (if any).\\n\", retriever_factories={'basic': <function _chroma_retriever_factory at 0x13f2a7820>, 'parent-doc': <function _chroma_parent_document_retriever_factory at 0x13f2a78b0>, 'hyde': <function _chroma_hyde_retriever_factory at 0x13f2a7940>}, architecture_factories={'conversational-retrieval-qa': <function default_response_chain at 0x139a454c0>}, get_docs=<function load_cached_docs at 0x139a45310>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langchain_docs = registry[\"LangChain Docs Q&A\"]\n",
    "langchain_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "888abae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset LangChain Docs Q&A already exists. Skipping.\n",
      "You can access the dataset at https://smith.langchain.com/o/cb5976d7-e8f5-5553-aca5-41aa75ce8690/datasets/cb68a322-4869-4e47-bc31-793a86e12074.\n"
     ]
    }
   ],
   "source": [
    "#os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "load_dotenv()\n",
    "\n",
    "clone_public_dataset(langchain_docs.dataset_id, dataset_name=langchain_docs.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5287c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document(page_content=\"LangChain cookbook | ðŸ¦œï¸ðŸ”— Langchain\\n\\n[Skip to main content](#docusaurus_skip...\n"
     ]
    }
   ],
   "source": [
    "docs = list(langchain_docs.get_docs())\n",
    "print(repr(docs[0])[:100] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e56ca1ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2051"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2cb8d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"thenlper/gte-base\",\n",
    "    # model_kwargs={\"device\": 0},  # Comment out to use CPU\n",
    ")\n",
    "\n",
    "vectorstore = Chroma(\n",
    "    collection_name=f\"lcbm-b-huggingface-gte-base\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./chromadb\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cdc31a74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a73502be-89e3-11ee-99ce-5e5044592b71']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore.add_documents([docs[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4ccb3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore.add_documents(docs)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 6})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d6b8852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChain Docs Q&A'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langchain_docs.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa70798d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(langchain_docs.get_docs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b611f82f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content=\"LangChain cookbook | ðŸ¦œï¸ðŸ”— Langchain\\n\\n[Skip to main content](#docusaurus_skipToContent_fallback)# LangChain cookbook\\n\\nExample code for building applications with LangChain, with an emphasis on more applied and end-to-end examples than contained in the [main documentation](https://python.langchain.com).\\n\\n| Notebook | Description |\\n| ---- | ---- |\\n| LLaMA2_sql_chat.ipynb | Build a chat application that interacts with a SQL database using an open source llm (llama2), specifically demonstrated on an SQLite database containing rosters. |\\n| Semi_Structured_RAG.ipynb | Perform retrieval-augmented generation (rag) on documents with semi-structured data, including text and tables, using unstructured for parsing, multi-vector retriever for storing, and lcel for implementing chains. |\\n| Semi_structured_and_multi_moda... | Perform retrieval-augmented generation (rag) on documents with semi-structured data and images, using unstructured for parsing, multi-vector retriever for storage and retrieval, and lcel for implementing chains. |\\n| Semi_structured_multi_modal_RA... | Perform retrieval-augmented generation (rag) on documents with semi-structured data and images, using various tools and methods such as unstructured for parsing, multi-vector retriever for storing, lcel for implementing chains, and open source language models like llama2, llava, and gpt4all. |\\n| analyze_document.ipynb | Analyze a single long document. |\\n| autogpt/autogpt.ipynb | Implement autogpt, a language model, with langchain primitives such as llms, prompttemplates, vectorstores, embeddings, and tools. |\\n| autogpt/marathon_times.ipynb | Implement autogpt for finding winning marathon times. |\\n| baby_agi.ipynb | Implement babyagi, an ai agent that can generate and execute tasks based on a given objective, with the flexibility to swap out specific vectorstores/model providers. |\\n| baby_agi_with_agent.ipynb | Swap out the execution chain in the babyagi notebook with an agent that has access to tools, aiming to obtain more reliable information. |\\n| camel_role_playing.ipynb | Implement the camel framework for creating autonomous cooperative agents in large-scale language models, using role-playing and inception prompting to guide chat agents towards task completion. |\\n| causalprogram_aided_language... | Implement the causal program-aided language (cpal) chain, which improves upon the program-aided language (pal) by incorporating causal structure to prevent hallucination in language models, particularly when dealing with complex narratives and math problems with nested dependencies. |\\n| code-analysis-deeplake.ipynb | Analyze its own code base with the help of gpt and activeloop's deep lake. |\\n| custom_agent_with_plugin_retri... | Build a custom agent that can interact with ai plugins by retrieving tools and creating natural language wrappers around openapi endpoints. |\\n| custom_agent_with_plugin_retri... | Build a custom agent with plugin retrieval functionality, utilizing ai plugins from theplugnplaidirectory. |\\n| databricks_sql_db.ipynb | Connect to databricks runtimes and databricks sql. |\\n| deeplakesemantic_search_over... | Perform semantic search and question-answering over a group chat using activeloop's deep lake with gpt4. |\\n| elasticsearch_db_qa.ipynb | Interact with elasticsearch analytics databases in natural language and build search queries via the elasticsearch dsl API. |\\n| extraction_openai_tools.ipynb | Structured Data Extraction with OpenAI Tools |\\n| forward_looking_retrieval_augm... | Implement the forward-looking active retrieval augmented generation (flare) method, which generates answers to questions, identifies uncertain tokens, generates hypothetical questions based on these tokens, and retrieves relevant documents to continue generating the answer. |\\n| generativeagents_interactive... | Implement a generative agent that simulates human behavior, based on a research paper, using a time-weighted memory object backed by a langchain retriever. |\\n| gymnasium_agent_simulation.ipynb | Create a simple agent-environment interaction loop in simulated environments like text-based games with gymnasium. |\\n| hugginggpt.ipynb | Implement hugginggpt, a system that connects language models like chatgpt with the machine learning community via hugging face. |\\n| hypothetical_document_embeddin... | Improve document indexing with hypothetical document embeddings (hyde), an embedding technique that generates and embeds hypothetical answers to queries. |\\n| learned_prompt_optimization.ipynb | Automatically enhance language model prompts by injecting specific terms using reinforcement learning, which can be used to personalize responses based on user preferences. |\\n| llm_bash.ipynb | Perform simple filesystem commands using language learning models (llms) and a bash process. |\\n| llm_checker.ipynb | Create a self-checking chain using the llmcheckerchain function. |\\n| llm_math.ipynb | Solve complex word math problems using language models and python repls. |\\n| llm_summarization_checker.ipynb | Check the accuracy of text summaries, with the option to run the checker multiple times for improved results. |\\n| llm_symbolic_math.ipynb | Solve algebraic equations with the help of llms (language learning models) and sympy, a python library for symbolic mathematics. |\\n| meta_prompt.ipynb | Implement the meta-prompt concept, which is a method for building self-improving agents that reflect on their own performance and modify their instructions accordingly. |\\n| multi_modal_output_agent.ipynb | Generate multi-modal outputs, specifically images and text. |\\n| multi_player_dnd.ipynb | Simulate multi-player dungeons & dragons games, with a custom function determining the speaking schedule of the agents. |\\n| multiagent_authoritarian.ipynb | Implement a multi-agent simulation where a privileged agent controls the conversation, including deciding who speaks and when the conversation ends, in the context of a simulated news network. |\\n| multiagent_bidding.ipynb | Implement a multi-agent simulation where agents bid to speak, with the highest bidder speaking next, demonstrated through a fictitious presidential debate example. |\\n| myscale_vector_sql.ipynb | Access and interact with the myscale integrated vector database, which can enhance the performance of language model (llm) applications. |\\n| openai_functions_retrieval_qa.... | Structure response output in a question-answering system by incorporating openai functions into a retrieval pipeline. |\\n| openai_v1_cookbook.ipynb | Explore new functionality released alongside the V1 release of the OpenAI Python library. |\\n| petting_zoo.ipynb | Create multi-agent simulations with simulated environments using the petting zoo library. |\\n| plan_and_execute_agent.ipynb | Create plan-and-execute agents that accomplish objectives by planning tasks with a language model (llm) and executing them with a separate agent. |\\n| press_releases.ipynb | Retrieve and query company press release data powered byKay.ai. |\\n| program_aided_language_model.i... | Implement program-aided language models as described in the provided research paper. |\\n| qa_citations.ipynb | Different ways to get a model to cite its sources. |\\n| retrieval_in_sql.ipynb | Perform retrieval-augmented-generation (rag) on a PostgreSQL database using pgvector. |\\n| sales_agent_with_context.ipynb | Implement a context-aware ai sales agent, salesgpt, that can have natural sales conversations, interact with other systems, and use a product knowledge base to discuss a company's offerings. |\\n| self_query_hotel_search.ipynb | Build a hotel room search feature with self-querying retrieval, using a specific hotel recommendation dataset. |\\n| smart_llm.ipynb | Implement a smartllmchain, a self-critique chain that generates multiple output proposals, critiques them to find the best one, and then improves upon it to produce a final output. |\\n| tree_of_thought.ipynb | Query a large language model using the tree of thought technique. |\\n| twitter-the-algorithm-analysis... | Analyze the source code of the Twitter algorithm with the help of gpt4 and activeloop's deep lake. |\\n| two_agent_debate_tools.ipynb | Simulate multi-agent dialogues where the agents can utilize various tools. |\\n| two_player_dnd.ipynb | Simulate a two-player dungeons & dragons game, where a dialogue simulator class is used to coordinate the dialogue between the protagonist and the dungeon master. |\\n| wikibase_agent.ipynb | Create a simple wikibase agent that utilizes sparql generation, with testing done onhttp://wikidata.org. |\", metadata={'changefreq': 'weekly', 'description': 'Example code for building applications with LangChain, with an emphasis on more applied and end-to-end examples than contained in the main documentation.', 'language': 'en', 'loc': 'https://python.langchain.com/cookbook', 'priority': '0.5', 'source': 'https://python.langchain.com/cookbook', 'title': 'LangChain cookbook | ðŸ¦œï¸ðŸ”— Langchain'})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "615d9723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"input\": \"in code, how can i add a system message at the end of the conversation history to influence the output of the llm\", \"ideal\": \"\\n```python\\nfrom langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\\n\\nprompt_template = ChatPromptTemplate.from_messages(\\n    [\\n        (\\\"human\\\", \\\"{user_input}\\\"),\\n        MessagesPlaceholder(variable_name=\\\"chat_history\\\")\\n        (\\\"system\\\", \\\"New System Instruction here\\\"),\\n    ]\\n)\\n```\\nWhen formatted, it will generate 1 human/user message and 1 system message, as well as messages for the whole conversation history.\\n```python\\nprompt_template.format_messages(user_input=\\\"Hello\\\", chat_history=[])\\n```\\n[HumanMessage(content='Hello', additional_kwargs={}, example=False),\\n SystemMessage(content='New System Instruction here', additional_kwargs={})]\\n\"}\n",
      " \n",
      "\n",
      "{\"input\": \"how to use enums in Tools that will be used by functions agent?\", \"ideal\": \"If you're defining a dynamic tool (either using the @tool decorator or the `.from_function(...) classmethod), simply using the enum typehint is enough. If you are manually specifying an `args_schema` as a pydantic basemodel, you can specify the enum as the argument typehint there.\"}\n",
      " \n",
      "\n",
      "{\"input\": \"How do I deal with openai rate limiting by having a backup model? Show me with code.\", \"ideal\": \"A straightforward way to do this is to specify fallback models using LangChain Expression Language. Example below:\\n```python     \\nfrom langchain.chat_models import ChatOpenAI\\n\\nbad_llm = ChatOpenAI(model_name=\\\"gpt-fake\\\")\\ngood_llm = ChatOpenAI(model_name=\\\"gpt-3.5-turbo\\\")\\nllm = bad_llm.with_fallbacks([good_llm])\\nllm.invoke(\\\"Why did the the chicken cross the road?\\\")\\n```\"}\n",
      " \n",
      "\n",
      "{\"input\": \"I'm runing my own model using vllm. How do I connect it to LangChain?\", \"ideal\": \"LangChain has a VLLM integration:\\\"\\n```python\\nfrom langchain.llms import VLLM\\n\\nllm = VLLM(model=\\\"mosaicml/mpt-7b\\\",\\n           trust_remote_code=True,  # mandatory for hf models\\n           max_new_tokens=128,\\n           top_k=10,\\n           top_p=0.95,\\n           temperature=0.8,\\n)\\n\\nprint(llm(\\\"What is the capital of France ?\\\"))\"}\n",
      " \n",
      "\n",
      "{\"input\": \"What's a runnable lambda?\", \"ideal\": \"A runnable lambda is an object that implements LangChain's `Runnable` interface and runs a callbale (i.e., a function). Note the function must accept a single argument.\"}\n",
      " \n",
      "\n",
      "{\"input\": \"What's an LLMChain\", \"ideal\": \"An LLMChain is a chain that composes basic LLM functionality. It consists of a PromptTemplate and a language model (either an LLM or chat model). It formats the prompt template using the input key values provided (and also memory key values, if available), passes the formatted string to LLM and returns the LLM output.\"}\n",
      " \n",
      "\n",
      "{\"input\": \"What's the difference between a prompt template and a chat prompt template?\", \"ideal\": \"A prompt template wraps instructions, examples, and other formatting information for the llm's prompts. A chat prompt template provides similar functionality for chat models. The largest difference is that chat prompt templates expose a `format_messages` method to convert the  prompt template into a list of structured chat messages.\"}\n",
      " \n",
      "\n",
      "{\"input\": \"What is ConversationSummaryBufferMemory?\", \"ideal\": \"ConversationSummaryBufferMemory is a memory type that combines the ideas of keeping a buffer of recent interactions and compiling them into a summary. It uses token length to determine when to flush interactions.\"}\n",
      " \n",
      "\n",
      "{\"input\": \"What does LangChain offer for token counting?\", \"ideal\": \"LangChain offers a context manager that allows you to count tokens, as well as integrations with services like LangSmith that can help track tokens.\"}\n",
      " \n",
      "\n",
      "{\"input\": \"What is the purpose of caching embeddings?\", \"ideal\": \"Caching embeddings allows for storing or temporarily caching embeddings to avoid the need for recomputing them.\"}\n",
      " \n",
      "\n",
      "{\"input\": \"What is the purpose of the Retry parser?\", \"ideal\": \"The Retry parser is used when the LLM output is not in the expected format. It passes the input and initial output back to the LLM so it can try again with the correct format.\"}\n",
      " \n",
      "\n",
      "{\"input\": \"What does ConversationBufferWindowMemory do?\", \"ideal\": \"ConversationBufferWindowMemory keeps a list of the interactions of the conversation over time. It only uses the last K interactions. This can be useful for keeping a sliding window of the most recent interactions, so the buffer does not get too large.\"}\n",
      " \n",
      "\n",
      "{\"input\": \"What is the main purpose of the time-weighted vector store retriever?\", \"ideal\": \"The main purpose of the time-weighted vector store retriever is to use a combination of semantic similarity and a time decay to retrieve relevant vectors.\"}\n",
      " \n",
      "\n",
      "{\"input\": \"What serialization format is used to serialize chains to and from disk?\", \"ideal\": \"The serialization format used is JSON or YAML.\"}\n",
      " \n",
      "\n",
      "{\"input\": \"whats an agent type that works well with Anthropic's models and supports structured output?\", \"ideal\": \"The XML agent works well with Anthropic's Claude 2 model. Here's some code for doing so:\\n\\n```python\\nfrom langchain.agents import XMLAgent, tool, AgentExecutor\\nfrom langchain.chat_models import ChatAnthropic\\nfrom langchain.chains import LLMChain\\nmodel = ChatAnthropic(model='claude-2')\\nchain = LLMChain(\\nllm=model,\\nprompt=XMLAgent.get_default_prompt(),\\noutput_parser=XMLAgent.get_default_output_parser()\\n)\\nagent = XMLAgent(tools=tool_list, llm_chain=chain)\\nagent_executor = AgentExecutor(agent=agent, tools=tool_list, verbose=True)\\n```\"}\n",
      " \n",
      "\n",
      "{\"input\": \"I want to save the configuration for a given LLM. Show me how to do that.\", \"ideal\": \"To save the configuration for a given LLM, you can follow these steps:\\n\\nFirst, let's go over loading an LLM configuration from disk. LLM configurations can be saved in two formats: JSON or YAML, but they are loaded in the same way.\\n\\nFor a JSON-formatted LLM configuration (llm.json):\\n\\n```json\\n{\\n    \\\"model_name\\\": \\\"text-davinci-003\\\",\\n    \\\"temperature\\\": 0.7,\\n    \\\"max_tokens\\\": 256,\\n    \\\"top_p\\\": 1.0,\\n    \\\"frequency_penalty\\\": 0.0,\\n    \\\"presence_penalty\\\": 0.0,\\n    \\\"n\\\": 1,\\n    \\\"best_of\\\": 1,\\n    \\\"request_timeout\\\": null,\\n    \\\"_type\\\": \\\"openai\\\"\\n}\\n```\\n\\nLoad the configuration like this:\\n\\n```python\\nfrom langchain.llms.loading import load_llm\\n\\nllm = load_llm(\\\"llm.json\\\")\\n```\\n\\nFor a YAML-formatted LLM configuration (llm.yaml):\\n\\n```yaml\\n_type: openai\\nbest_of: 1\\nfrequency_penalty: 0.0\\nmax_tokens: 256\\nmodel_name: text-davinci-003\\nn: 1\\npresence_penalty: 0.0\\nrequest_timeout: null\\ntemperature: 0.7\\ntop_p: 1.0\\n```\\n\\nLoad the configuration like this:\\n\\n```python\\nfrom langchain.llms.loading import load_llm\\n\\nllm = load_llm(\\\"llm.yaml\\\")\\n```\\n\\nTo save an LLM configuration from memory to a serialized version, you can use the `.save` method. This method supports both JSON and YAML formats:\\n\\n```python\\nllm.save(\\\"llm.json\\\")\\nllm.save(\\\"llm.yaml\\\")\\n```\\n\\nThis way, you can easily save and load LLM configurations to and from disk.\"}\n",
      " \n",
      "\n",
      "{\"input\": \"Show me an example using Weaviate, but customizing the VectorStoreRetriever to return the top 10 k nearest neighbors. \", \"ideal\": \"To customize the Weaviate client and return the top 10 k nearest neighbors, you can utilize the `as_retriever` method with the appropriate parameters. Here's how you can achieve this:\\n\\n```python\\n# Assuming you have imported the necessary modules and classes\\n\\n# Create the Weaviate client\\nclient = weaviate.Client(url=os.environ[\\\"WEAVIATE_URL\\\"], ...)\\n\\n# Initialize the Weaviate wrapper\\nweaviate = Weaviate(client, index_name, text_key)\\n\\n# Customize the client to return top 10 k nearest neighbors using as_retriever\\ncustom_retriever = weaviate.as_retriever(\\n    search_type=\\\"similarity\\\",\\n    search_kwargs={\\n        'k': 10  # Customize the value of k as needed\\n    }\\n)\\n\\n# Now you can use the custom_retriever to perform searches\\nresults = custom_retriever.search(query, ...)\\n```\"}\n",
      " \n",
      "\n",
      "{\"input\": \"What is BabyAGI\", \"ideal\": \"BabyAGI is an example of an \\\"autonomous AI agent\\\" that can generate and simulate the execution of tasks based on a given objective.\"}\n",
      " \n",
      "\n",
      "{\"input\": \"What is the difference between ChatPromptTemplate and PromptTemplate?\", \"ideal\": \"ChatPromptTemplate and PromptTemplate are similar in their functions. PromptTemplate takes a prompt as a string that can contain variables. ChatPromptTemplate separates the prompt into structured messages of different kinds. Each chat message is associated with content, and an additional parameter called role. Some example roles include system, human (or user), and AI (or assistant).\"}\n",
      " \n",
      "\n",
      "{\"input\": \"Show me how to use RecursiveURLLoader\", \"ideal\": \"The RecursiveURLLoader comes from the langchain.document_loaders.recursive_url_loader module. Here's an example of how to use it:\\n\\n```python\\nfrom langchain.document_loaders.recursive_url_loader import RecursiveUrlLoader\\n\\n# Create an instance of RecursiveUrlLoader with the URL you want to load\\nloader = RecursiveUrlLoader(url=\\\"https://example.com\\\")\\n\\n# Load all child links from the URL page\\ndocuments = loader.load()\\n\\n# Fetch the documents\\nfor doc in documents:\\n    print(doc)\\n```\\n\\nMake sure to replace \\\"https://example.com\\\" with the actual URL you want to load.\"}\n",
      " \n",
      "\n",
      "{\"input\": \"what are the main methods supported by Runnables\", \"ideal\": \"The `runnable` interface in LangChain supports several main methods for interacting with components:\\n\\n1. `stream`: This method streams back response chunks in real-time, useful for processing large computations or text generation as they become available.\\n\\n2. `invoke`: Calls the runnable chain on a single input, providing a single response output.\\n\\n3. `batch`: Calls the runnable chain on a list of inputs, returning a list of corresponding responses.\\n\\n4. `astream`: Asynchronously streams response chunks back, suitable for handling concurrent tasks.\\n\\n5. `ainvoke`: Asynchronously calls the runnable chain on a single input and returns the response.\\n\\n6. `abatch`: Asynchronously calls the runnable chain on a list of inputs, returning a list of corresponding responses.\"}\n",
      " \n",
      "\n",
      "{\"input\": \"What is html2texttransformer? Does it omit urls?\", \"ideal\": \"The `Html2TextTransformer` is a class provided by LangChain's `document_transformers` module. It is used to transform HTML content into clean, easy-to-read plain ASCII text.\\n\\nThis transformer is designed to convert HTML pages into human-readable text, making it suitable for scenarios where the goal is to extract content without the need to manipulate specific HTML elements.\\n\\nIf urls are present in the source Document's metadata, then they will be retained. If they are stored in the page_content itself, the class makes no guarantees that they will be retained.\"}\n",
      " \n",
      "\n",
      "{\"input\": \"I want to return the source documents of my Weaviate retriever. Show me how\", \"ideal\": \"To retrieve the source documents from your Weaviate vectorstore, you need to configure your client to include the 'sources' attribute. Here's an example of how you can achieve this:\\n\\n```python\\nweaviate_client = Weaviate(\\n    client=client,\\n    index_name='index_name',\\n    text_key='text',\\n    embedding=OpenAIEmbeddings(),\\n    by_text=False,\\n    attributes=['source'],\\n)\\n```\\n\\nBy including the 'source' attribute in the attributes list, you ensure that the source documents are returned when you interact with the Weaviate client.\"}\n",
      " \n",
      "\n",
      "{\"input\": \"what is RAG\", \"ideal\": \"RAG stands for Retrieval Augmented Generation, which refers to the application design of using an LLM to generate responses to user inputs grounded on retrieved information. The retrieved information can come from anywhere, though it typically is fetched from 1 or more databases, search engines, knowledge graphs, or other sources.\"}\n",
      " \n",
      "\n",
      "{\"input\": \"whats the code to load text file into a vector store\", \"ideal\": \"To load a text file into a vector store using LangChain's document loaders, you can use the following code snippet:\\n\\n```python\\nfrom langchain.document_loaders import TextLoader\\n\\nloader = TextLoader(\\\"./index.txt\\\")\\ndocs = loader.load()\\nprint(docs)\\n```\\n\\nThis code imports the `TextLoader` class from the document_loaders module, creates an instance of the loader with the path to the text file as an argument, and then uses the `load()` method to read the file's content and create a Document. The resulting Document will contain the text content of the file.\"}\n",
      " \n",
      "\n",
      "{\"input\": \"Help me debug: TypeError: Pinecone.similarity_search_with_score() got an unexpected keyword argument 'min_score'\", \"ideal\": \"Your error is likely due to an incorrect parameter being passed to the 'similarity_search_with_score' function in the Pinecone vector store. The correct function signature does not include a 'min_score' argument. This function retrieves documents most similar to a given query text while providing both the documents and their corresponding similarity scores. To use 'similarity_search_with_score', ensure you provide the right arguments: 'query' for the text you want to find similar documents for, 'k' for the number of documents to retrieve (default: 4), 'filter' as an optional metadata-based filtering dictionary, and 'namespace' to specify the search namespace (default is the empty namespace ''). The function returns a list of tuples, each containing a Document and a similarity score (float). To resolve the 'TypeError' issue, ensure proper argument usage as per the documentation.  \"}\n",
      " \n",
      "\n",
      "{\"input\": \"How can I create a simple chat model using my locally saved huggingface model\", \"ideal\": \"Certainly! To create a simple chat model using your locally saved Hugging Face model, follow these steps:\\n\\n1. Install the necessary Python libraries:\\n\\n- Install 'transformers' library for working with models and tokenizers using 'pip install transformers'.\\n\\n2. Import the `HuggingFacePipeline` class to work with your local pipeline\\n\\n```\\nfrom langchain.llms import HuggingFacePipeline\\n\\n# Create an instance of the wrapper\\nllm = HuggingFacePipeline.from_model_id(\\\"path/to/model_directory\\\", task=\\\"text-generation\\\")\\n```\\nAlternatively, load your pipeline directly:\\n```\\nfrom langchain.llms.huggingface_pipeline import HuggingFacePipeline\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\\n\\nmodel_path = \\\"path/to/model_dir\\\"\\ntokenizer = AutoTokenizer.from_pretrained(model_path)\\nmodel = AutoModelForCausalLM.from_pretrained(model_path)\\npipe = pipeline(\\n    \\\"text-generation\\\", model=model, tokenizer=tokenizer, max_new_tokens=10\\n)\\nllm = HuggingFacePipeline(pipeline=pipe)\\n```\\n\\n Finally, interact with the chat model\\n```\\nresponse = llm.invoke('User: Hello, how can I help?')\\n```\"}\n",
      " \n",
      "\n",
      "{\"input\": \"How do I use a React Agent with an Anthropic model?\", \"ideal\": \"In order to use a ReAct Agent with an Anthropic model, you should first install the reuqired packages:\\n\\npip install anthropic\\n\\nNext, import the necessary classes and functions:\\n\\n    from langchain.agents import load_tools\\n    from langchain.agents import initialize_agent\\n    from langchain.agents import AgentType\\n    from langchain.chat_models import ChatAnthropic\\n\\nNext, create an instance of ChatAnthropic. Make sure you have set your API key in an environment variable called ANTHROPIC_API_KEY or pass anthropic_api_key as a parameter.\\n\\n    chat = ChatAnthropic()\\n\\nNext, load some tools for your agent to use:\\n\\n    # Specify tools (these can be any custom tool, the following are examples)\\ntools = load_tools([\\\"serpapi\\\", \\\"llm-math\\\"], llm=chat)\\n\\nFinally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\\n\\n    agent = initialize_agent(tools, chat, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\\n\\nNow, enter a prompt to test it out:\\n\\n    agent.invoke(\\\"Who is Leo DiCaprio's girlfriend? What is her current age raised to the 0.43 power?\\\")\"}\n",
      " \n",
      "\n",
      "{\"input\": \"How do I use Qdrant as a vector store in the conversational retrieval chain?\", \"ideal\": \"In order to use Qdrant as a vector store in the conversational retrieval chain, you should start by installing Qdrant:\\n\\n    pip install qdrant-client\\n\\nNext, import the necessary classes and functions:\\n\\n    from langchain.vectorstores import Qdrant\\n    from langchain.embeddings.openai import OpenAIEmbeddings\\n    from langchain.text_splitter import CharacterTextSplitter\\n    from langchain.llms import OpenAI\\n    from langchain.chains import ConversationalRetrievalChain\\n    from langchain.document_loaders import TextLoader\\n    from langchain.memory import ConversationBufferMemory\\n\\nLoad, split, and embed your documents:\\n\\n    loader = TextLoader(\\\"state_of_the_union.txt\\\")\\n    documents = loader.load()\\n\\n    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0, separator=\\\"\\n\\\")\\n    documents = text_splitter.split_documents(documents)\\n    embeddings = OpenAIEmbeddings()\\n\\nNext, connect to Qdrant. This is for storing locally on-disk, but you can also use an on-premise server deployment or store on the Qdrant cloud:\\n\\n    qdrant = Qdrant.from_documents(\\n        documents,\\n        embeddings,\\n        path=\\\"/tmp/local_qdrant\\\",\\n        collection_name=\\\"my_documents\\\",\\n    )\\n\\nNext, create a memory object, which is necessary to track the inputs/outputs and hold a conversation:\\n\\n    memory = ConversationBufferMemory(memory_key=\\\"chat_history\\\", return_messages=True)\\n\\nNow initialize the ConversationalRetrievalChain:\\n\\n    qa = ConversationalRetrievalChain.from_llm(OpenAI(temperature=0), qdrant.as_retriever(), memory=memory)\\n\\nNow, you can create queries to test it out:\\n\\n    query = \\\"What did the president say about Ketanji Brown Jackson\\\"\\n    result = qa({\\\"question\\\": query})\\n\\nOutput:\\n\\n    The president said that he had nominated Ketanji Brown Jackson to serve on the United States Supreme Court four days ago.\\n\\n    query = \\\"Who are they succeeding?\\\"\\n    result = qa({\\\"question\\\": query})\\n\\nOutput:\\n\\n    Justice Breyer.\"}\n",
      " \n",
      "\n",
      "{\"input\": \"How do I use Summary Memory in the conversational retrieval chain?\", \"ideal\": \"In order to use Summary Memory in the conversational retrieval chain, you should first import the necessary classes and functions:\\n\\n        from langchain.memory import ConversationSummaryMemory\\n    from langchain.embeddings.openai import OpenAIEmbeddings\\n    from langchain.vectorstores import Chroma # You can choose any LangChain vectorstore\\n    from langchain.text_splitter import CharacterTextSplitter\\n    from langchain.chat_models import ChatOpenAI # You can choose any LangChain vectorstore\\n    from langchain.chains import ConversationalRetrievalChain\\n    from langchain.document_loaders import TextLoader\\n\\nSplit and embed your documents, and add them to your selected vectorstore:\\n\\n    loader = TextLoader(\\\"state_of_the_union.txt\\\")\\n    documents = loader.load()\\n\\n    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0, separator=\\\"\\n\\\")\\n    documents = text_splitter.split_documents(documents)\\n    embeddings = OpenAIEmbeddings()\\n    vectorstore = Chroma.from_documents(documents, embeddings)\\n\\nNext, create a ConversationSummaryMemory object:\\n\\n    memory = ConversationSummaryMemory(llm=ChatOpenAI(model=\\\"gpt-3.5-turbo\\\"), memory_key=\\\"chat_history\\\", return_messages=True)\\n\\nNow initialize the ConversationalRetrievalChain:\\n\\n    qa = ConversationalRetrievalChain.from_llm(ChatOpenAI(model=\\\"gpt-3.5-turbo\\\",temperature=0), vectorstore.as_retriever(), memory=memory)\\n\\nNow, you can create queries to test it out:\\n\\n    query = \\\"What did the president say about Ketanji Brown Jackson\\\"\\n    result = qa({\\\"question\\\": query})\\n\\nOutput:\\n\\n    The president said that he had nominated Ketanji Brown Jackson to serve on the United States Supreme Court four days ago.\\n\\n    query = \\\"Who are they succeeding?\\\"\\n    result = qa.invoke({\\\"question\\\": query})\\n\\nOutput:\\n\\n    Justice Breyer.\"}\n",
      " \n",
      "\n",
      "{\"input\": \"How do I load Youtube transcripts and CSV documents?\", \"ideal\": \"To load YouTube transcripts in LangChain, you can use the YoutubeLoader document loader. Here's an example of how to do it:\\n\\nInstall the required packages:\\n```\\npip install langchain youtube_transcript_api pytube\\n```\\n\\nImport the necessary classes:\\n\\n\\n    from langchain.document_loaders import YoutubeLoader\\n\\n\\nCreate an instance of the YoutubeLoader class and provide the YouTube video URL:\\n\\n\\n    loader = YoutubeLoader.from_youtube_url(\\\"https://www.youtube.com/watch?v=QsYGlZkevEg\\\", add_video_info=True)\\n\\n\\nLoad the transcript using the load method:\\n\\n    video = loader.load()\\n\\nTo load CSV documents you can use CSVLoader. Follow the steps below:\\n\\nImport the necessary classes:\\n\\n    from langchain.document_loaders import CSVLoader\\n\\n\\nCreate an instance of the CSVLoader class and provide the path to the CSV file:\\n\\n\\n    loader = CSVLoader(file_path=\\\"./example_data/mlb_teams_2012.csv\\\")\\n\\nLoad the CSV document using the load method:\\n\\n    loader.load()\"}\n",
      " \n",
      "\n",
      "{\"input\": \"my agent keeps getting an OutputParserException is something i can set to make it take care of these?\", \"ideal\": \"You can control the agent's behavior for output parsing errors by initializing the agent with `handle_parsing_errors`=True. Then the agent will see 'Handle errors with Invalid or incomplete response'. You can also set a string to provide a custom error message to the agent, or provide a custom error handling function.\"}\n",
      " \n",
      "\n",
      "{\"input\": \"What does ReAct mean?\", \"ideal\": \"ReAct in LangChain refers to a type of agent that 'Reasons' and 'Act's in a loop.\"}\n",
      " \n",
      "\n",
      "{\"input\": \"What are intermediate steps in langchain?\", \"ideal\": \"Intermediate stps represent previous agent actions and corresponding outputs. They are used to help the agent know what work has already been done in pursuit of a goal.\"}\n",
      " \n",
      "\n",
      "{\"input\": \"im getting a rate limit error for my llm - how do I have it automatically go to a different model?\", \"ideal\": \"To switch to a different model when an LLM is gettin rate limited, you can use 'fallbacks'. For instance, if you have too llms: openai_llm and anthropic_llm, you can have the program fall back to the anthropic llm by calling `llm = openai_llm.with_fallbacks([anthropic_llm])`\"}\n",
      " \n",
      "\n",
      "{\"input\": \"What's function calling\", \"ideal\": \"Function calling refers to the ability to define functions for LLMs to let them choose to call with structured arguments.\"}\n",
      " \n",
      "\n",
      "{\"input\": \"what is the langserve\", \"ideal\": \"LangServe is an easy way to deploy any LangChain runnable, chain, or agent. It uses FastAPI and pydantic to automatically create an API server from any compatible LangChain component.\"}\n",
      " \n",
      "\n",
      "{\"input\": \"What's a string evaluator\", \"ideal\": \"A string evaluator in LangChain is an object that assesses the performance of a language model's predicted response by comparing it to an input string and/or reference string.\"}\n",
      " \n",
      "\n",
      "{\"input\": \"whats a toolkit\", \"ideal\": \"Toolkits are collections of tools that are designed to be used together for specific tasks and have convenience loading methods.\"}\n",
      " \n",
      "\n",
      "{\"input\": \"is langchain compatible with pydantic v2?\", \"ideal\": \"As of langchain>=0.0.267, LangChain will allow users to install either Pydantic V1 or V2, though it internally will use pydantic V1.\"}\n",
      " \n",
      "\n",
      "{\"input\": \"How can I parallelize calls in LangChain?\", \"ideal\": \"To make parallel calls from a LangChain object, use the 'batch()' (or asynchronous 'abatch()') method. You can also use a `RunnableParallel` object.\"}\n",
      " \n",
      "\n",
      "{\"input\": \"how many llm api calls are made in OpenAIFunctionsAgent\", \"ideal\": \"The OpenAIFunctionsAgent is an agent driven by OpenAI's function-powered API. It uses the function_call parameter from OpenAI to determine which tools to use for each API call. In general, the number of API calls made by the OpenAIFunctionsAgent depends on the question and the number of tools used in the agent's pipeline. At minimum, one call will be made to the LLM to (optionally) select a tool or directly respond. For more complex questions or intents, the agent may make many calls.\"}\n",
      " \n",
      "\n",
      "{\"input\": \"What are some ways of doing retrieval augmented generation?\", \"ideal\": \"Retrieval Augmented Generation (RAG) is a process that involves retrieving external data and using it in the generation step of language models. LangChain provides various building blocks for RAG applications. Here are some ways of doing retrieval augmented generation:\\n\\n- Document loaders: LangChain offers over 100 document loaders to load documents from different sources, including private S3 buckets and public websites.\\n- Document transformers: These help in fetching relevant parts of documents by splitting or chunking large documents into smaller chunks.\\n- Text embedding models: LangChain integrates with over 25 different embedding providers and methods to create embeddings for documents, allowing efficient similarity-based retrieval.\\n- Vector stores: With the rise of embeddings, LangChain supports over 50 vector stores for efficient storage and searching of embeddings.\\n- Retrievers: LangChain supports various retrieval algorithms, including semantic search, parent document retriever, self-query retriever, ensemble retriever, and more, to retrieve the data from the database.\\n\\nThese components work together to enable retrieval augmented generation in LangChain. [0] [1]\"}\n",
      " \n",
      "\n",
      "{\"input\": \"whats the difference between a handler and an inheritable_handler?\", \"ideal\": \"In Langchain, both `handler` and `inheritable_handler` are types of callback handlers. Handlers can refer to inheritable handlers or local handlers. Inheritable handlers are propagated to other objects that are called within a Runnable, chain, tool, or other object. Local handlers are only called by the object they are assigned to and not called by any other objects.\"}\n",
      " \n",
      "\n",
      "{\"input\": \"What is a chain?\", \"ideal\": \"A 'Chain' is a class in LangChain that composes a number of components together into a single object. An example would be the LLMChain, which formats an input as a prompt, calls an LLM, and then parses the output. \\\"chaining\\\" also can generically refer to composing llms, functions, and other operations together into a larger program.\"}\n",
      " \n",
      "\n",
      "{\"input\": \"What is LangChain Expression Language?\", \"ideal\": \"LangChain Expression Language (LCEL) is a declarative way to easily compose chains together. It provides several benefits for building applications:\\n\\n- Async, Batch, and Streaming Support: LCEL chains have full sync, async, batch, and streaming support, making it easy to prototype in a Jupyter notebook and then expose it as an async streaming interface.\\n- Fallbacks: LCEL allows you to easily attach fallbacks to any chain, handling errors gracefully.\\n- Parallelism: LCEL syntax automatically runs components that can be run in parallel.\\n- Seamless LangSmith Tracing Integration: LCEL logs all steps to LangSmith for observability and debuggability. [0]\\n\\nLCEL provides an intuitive and readable syntax for composition and supports streaming, async calls, batching, parallelization, retries, and more. It allows you to combine different components, such as prompts, models, and output parsers, to build powerful applications. LCEL chains can be called synchronously or asynchronously, and they support intermediate result access, input/output schemas, and tracing with LangSmith. [3]\"}\n",
      " \n",
      "\n",
      "{\"input\": \"which document laaoder should i use for a loading a single web apage?\", \"ideal\": \"For a single web page, the WebBaseLoader or AsyncHtmlLoader should suffice. If you want to load multiple pages by traversing a whole website, you can use the SitemapLoader or RecursiveUrlLoader.\"}\n",
      " \n",
      "\n",
      "{\"input\": \"whats the difference between run_id and example_id\", \"ideal\": \"The `run_id` and `example_id` are both identifiers used in LangChain and LangSmith, but they serve different purposes. The `run_id` is a unique ID assigned to each individual \\\"run\\\" in a trace, which represents a unit of work whenever a runnable object such as an LLM or chain is called. An `example_id` is a unique ID assigned to each row (or \\\"example\\\") in a LangSmith dataset.\"}\n",
      " \n",
      "\n",
      "{\"input\": \"What is an agent\", \"ideal\": \"An agent is a component in the Langchain framework that uses a language model (LLM) to determine which actions to take and in what order. It is responsible for making decisions and interacting with tools to accomplish specific objectives.\"}\n",
      " \n",
      "\n",
      "{\"input\": \"how do I search and filter metadata in redis vectorstore?\", \"ideal\": \"To perform a similarity search with metadata filtering in LangChain using the Redis vectorstore, you can use Redis filter expressions. The following filter operations are available: RedisText, RedisNum, and RedisTag. Use these when calling `similarity_search()`.\"}\n",
      " \n",
      "\n",
      "{\"input\": \"how do I control the maximum number requests that can be made at the same time when making batch calls?\", \"ideal\": \"To limit the maximum number of concurrent requests for batch calls in LangChain, you can call the `.batch()` method and configure the 'max_concurrency' paramater within the runnable config.  For example, the following code calls the chain with a maximum of 2 concurrent calls:\\n```python\\nchain.batch([{'topic': 'bears'}, {'topic': 'cats'}, {'topic': 'dogs'}], config={'max_concurrency': 2})\\n```\"}\n",
      " \n",
      "\n",
      "{\"input\": \"can i cache LLM calls in sqlite?\", \"ideal\": \"To add LLM caching in sqlite, you can use the SQLiteCache class. For example:\\n```python\\nfrom langchain.globals import set_llm_cache\\nfrom langchain.cache import SQLiteCache\\nfrom langchain.chat_models import ChatOpenAI\\n\\nllm = ChatOpenAI()\\n\\nset_llm_cache(SQLiteCache(database_path=\\\".langchain.db\\\"))\\nllm.invoke(\\\"Tell me a joke\\\")\\n```\"}\n",
      " \n",
      "\n",
      "{\"input\": \"how can I create a vectorstore from the texts in the list\", \"ideal\": \"Each LangChain vectorstore class implements a \\\"from_texts\\\" class method that accepts a list of string texts, an instance of an `Embeddings` object to embed those texts, and an optional list of metadata dictionaries to add to the resulting documents.\"}\n",
      " \n",
      "\n",
      "{\"input\": \"what does on_tool_start mean?\", \"ideal\": \"The on_tool_start method is defined classes that implement LangChain's BaseCallbackHandler interface. When a \\\"Tool\\\" object is invoked, and callbacks are provided, the 'on_tool_start' method is called prior to executing the tool itself.\"}\n",
      " \n",
      "\n",
      "{\"input\": \"what does this do? return RunnableBranch(\\n        (\\n            RunnableLambda(lambda x: bool(x.get(\\\"chat_history\\\"))).with_config(\\n                run_name=\\\"HasChatHistoryCheck\\\"\\n            ),\\n            conversation_chain.with_config(run_name=\\\"RetrievalChainWithHistory\\\"),\\n        ),\\n        (\\n            RunnableLambda(itemgetter(\\\"question\\\")).with_config(\\n                run_name=\\\"Itemgetter:question\\\"\\n            )\\n            | retriever\\n        ).with_config(run_name=\\\"RetrievalChainWithNoHistory\\\"),\\n    ).with_config(run_name=\\\"RouteDependingOnChatHistory\\\")\", \"ideal\": \"This code defines a runnable chain with two branches. The first branch checks if the input has a 'chat_history' key and runs the conversation_chain if it does. Otherwise uses python stdlib's \\\"itemgetter\\\" to get the \\\"question\\\" item from the input dictionary. Each \\\"with_config\\\" call assigns a friendly \\\"run_name\\\" to that step in the traced DAG.\"}\n",
      " \n",
      "\n",
      "{\"input\": \"How can I use OpenAI functions to get structured outputs in a chain?\", \"ideal\": \"You can use OpenAI functions to get structured outputs in a chain by using the create_structured_output_runnable function. This function takes the desired structured output either as a Pydantic class or as JsonSchema. If you pass in a model explicitly, make sure it supports the OpenAI function-calling API. You can then invoke the runnable with the input data, and it will return the structured output. You can also use the bind_functions() method on the ChatOpenAI class to do this directly from an instance of ChatOpenAI.\"}\n",
      " \n",
      "\n",
      "{\"input\": \"hw do i create a prompt template for my chat bot\", \"ideal\": \"To create a prompt template for a chat bot, you can use the `ChatPromptTemplate` class. You can initialize it with a list of chat messages, where each message has a role and content. The role can be 'system', 'human', or 'ai'. For example:\\n\\n```python\\nfrom langchain.prompts import ChatPromptTemplate\\n\\nchat_template = ChatPromptTemplate.from_messages(\\n    [\\n        ('system', 'You are a helpful AI bot. Your name is {name}.'),\\n        ('human', 'Hello, how are you doing?'),\\n        ('ai', 'I'm doing well, thanks!'),\\n        ('human', '{user_input}'),\\n    ]\\n)\\n\\nmessages = chat_template.format_messages(name='Bob', user_input='What is your name?')\\n```\\n\\nYou can also use different subclasses of the `ChatMessagePromptTemplate` class, such as `SystemMessagePromptTemplate` and `HumanMessagePromptTemplate`, to customize the role or content of specific messages in the template. Finally, you can use the \\\"MessagesPlaceholder\\\" class to format chat history as a sequence of messages.\"}\n",
      " \n",
      "\n",
      "{\"input\": \"what method should subclasses override if they can start producing output while input is still being generated\", \"ideal\": \"Subclasses should override the transform() method if they can start producing output while input is still being generated.\"}\n",
      " \n",
      "\n",
      "{\"input\": \"what does runnable mean\", \"ideal\": \"In the context of LangChain, runnable's are the building blocks of the LangChain Expression Language. They implement the \\\"Runnable\\\" base class and represent a unit of work that can be invoked, batched, streamed, transformed and composed.\"}\n",
      " \n",
      "\n",
      "{\"input\": \"I am summarizing text contained in the variable chunks with load_summarize_chain.\\n\\nchain = load_summarize_chain(llm, chain_type=\\\"map_reduce\\\")\\nchain.run(chunks)\\nI would like to add a tag when I run the chain that langsmith will capture. How?\", \"ideal\": \"all chains inherit from the runnable interface. So they support an invoke method. The invoke method takes the input and a config.\\nFor the config you can pass in { 'tags': [ 'tag1' ] }.\\n\\nchain.invoke( input, config={ 'tags': ['tags1'] }.\\n\\nIf you want to just tag that run (and not propagate the tags to child calls), you can pass the tags in as a list of strings to the call to `load_summarize_chain(llm, chain_type=\\\"map_reduce\\\", tags=['tag1'])`.\\nFor older methods in the LangChain API like run, __call__, etc. you can also add tags directly when calling, such as with `chain.run(\\\"foo\\\", tags=['tag1])`.\"}\n",
      " \n",
      "\n",
      "{\"input\": \"Let's say I have a chain like:\\n\\nmodel_call_1 = (\\n  RunnablePassthrough()\\n  | prompt\\n  | model_parser\\n)\\n# ...\\nchain = model_call_1 #| { \\\"attr\\\": model_call_2 } | model_call_3\\nHow can I print out the filled out prompts for each model call?\", \"ideal\": \"There are a few options. You can use set_debug(True) to print out each run to the console:\\n```\\nfrom langchain.globals import set_debug\\n\\nset_debug(True)\\n```\\nThis uses the ConsoleCallbackHandler. You can also trace with LangSmith by configuring your environment:\\n\\n```\\nexport LANGCHAIN_API_KEY=\\\"Your API Key\\\"\\nexport LANGCHAIN_TRACING_V2=true\\n```\"}\n",
      " \n",
      "\n",
      "{\"input\": \"soooo, is it possible to pass any kwargs for similarity thresholds or K-top documents in the multivectorretriever as we do in normal retrievers?\", \"ideal\": \"You can assign search_kwargs (such as k) to the instance of the MultiVectorRetriever\"}\n",
      " \n",
      "\n",
      "{\"input\": \"whats the stuff documents chain?\", \"ideal\": \"The stuff documents chain (\\\"stuff\\\" as in \\\"to stuff\\\" or \\\"to fill\\\") is the most straightforward of the document chains. It takes a list of documents, inserts them all into a prompt and passes that prompt to an LLM.\\n\\nThis chain is well-suited for applications where documents are small and only a few are passed in for most calls.\"}\n",
      " \n",
      "\n",
      "{\"input\": \"Does AlephAlpha support token-level streaming natively in LangChain?\", \"ideal\": \"No, while the stream() method is provided in all langchain runnables, AlephAlpha doesn't natively implement streaming on the token level.\"}\n",
      " \n",
      "\n",
      "{\"input\": \"How to use a prompt template with chat history\", \"ideal\": \"To add chat history to a prompt template, you can use the MessagesPlaceholder class.\\n\\n```\\nfrom langchain.prompts import MessagesPlaceholder, ChatPromptTemplate\\n\\nchat_template = ChatPromptTemplate.from_messages(\\n    [\\n        (\\\"system\\\", \\\"You are a helpful AI bot.\\\"),\\n        MessagesPlaceholder(variable_name=\\\"chat_history\\\")\\n        (\\\"human\\\", \\\"{input}\\\"),\\n    ]\\n)\\n```\\n\"}\n",
      " \n",
      "\n",
      "{\"input\": \"How do i run llama 2 in langchain\", \"ideal\": \"Llama 2 can be run through a number of LangChain llm and chat model integrations. A non-exhaustive list includes LlamaCPP, Ollama, Huggingface, Fireworks, and Replicate, HuggingFace. These imports would be\\n```python\\nfrom langchain.llms import Fireworks, HuggingFacePipeline, LlamaCpp, Ollama, Replicate, HuggingFaceHub, HuggingFaceTextGenInference, HuggingFaceEndpoint\\nfrom langchain.chat_models import ChatFireworks, ChatOllama\\n```\\n\\nInitializing any of these classes with a parameter pointing to the llama2 model should suffice. Noteably, there is no LangChain class called Llama2, Llama, or Huggingface (with no suffixes). The classes listed above are also case-sensitive.\"}\n",
      " \n",
      "\n",
      "{\"input\": \"What's the difference between the OpenAI and ChatOpenAI classes?\", \"ideal\": \"OpenAI wraps OpenAI's completions endpoint, whereas the ChatOpenAI class wraps the chat endpoint. Both interfadce with LLM's, but the interface chat models is structured as chat messages, which contain information about the role of speaker. Examples of roles include system, human/user, and AI/assistant.\"}\n",
      " \n",
      "\n",
      "{\"input\": \"how do i run llama on vllm\", \"ideal\": \"You can run HuggingFace models in vLLM, so you can select the model name of a compatible Llama model then use the VLLM class in LangChain:\\n ```python\\nfrom langchain.llms import VLLM\\n\\nllm = VLLM(\\n    model=\\\"name-of-llama-model\\\",\\n    trust_remote_code=True,  # mandatory for hf models\\n)\\n\\nprint(llm(\\\"What is the capital of France ?\\\"))\\n```\"}\n",
      " \n",
      "\n",
      "{\"input\": \"What class type is returned by initialize_agent\", \"ideal\": \"AgentExecutor\"}\n",
      " \n",
      "\n",
      "{\"input\": \"what does runnable.predict() mean?\", \"ideal\": \"The runnable base class does not have a predict() method, but the LLM classes (which inherit from Runnable) do. For those classes, predict() formats a prompt with keyword args and passes it to the LLM.\"}\n",
      " \n",
      "\n",
      "{\"input\": \"how to run a runnable\", \"ideal\": \"Use invoke(), stream(), batch(), or their async equivalents (ainvoke, astream, and abatch)\"}\n",
      " \n",
      "\n",
      "{\"input\": \"how do I run gpt-4 on anthropic?\", \"ideal\": \"GPT-4 is a model trained by OpenAI and not provided by Anthropic.\"}\n",
      " \n",
      "\n",
      "{\"input\": \"who to email for filing cves\", \"ideal\": \"security@langchain.dev\"}\n",
      " \n",
      "\n",
      "{\"input\": \"What class is made when using | in langchain?\", \"ideal\": \"The | pipe operator combines (or coerces) two runnables ( or a runnable and a function or dict or other compatible primitive) into a new runnable, namely a \\\"RunnableSequence.\\\"\"}\n",
      " \n",
      "\n",
      "{\"input\": \"How do i run gpt-4 locally?\", \"ideal\": \"GPT-4 is a closed-source model provided through an API by OpenAI. While you can connect to GPT-4 via the API, you cannot run the model locally.\"}\n",
      " \n",
      "\n",
      "{\"input\": \"How do i run lcel in java\", \"ideal\": \"LangChain, along with LCEL, or LangChain Expression Language, is not natively offered in java. LangChain is available in both python and javascript/typescript.\"}\n",
      " \n",
      "\n",
      "{\"input\": \"What's the difference between a document loader and a chat loader?\", \"ideal\": \"A document loader is used to load data as Document objects, which are pieces of text with associated metadata. A chat loader is specifically designed to load chat messages from different platforms, such as Discord, Facebook Messenger, Slack, and Telegram. Chat loaders load these conversations as ChatSessions, which contain a list of BaseMessage's\"}\n",
      " \n",
      "\n",
      "{\"input\": \"whats the diff between a docstore and a vector store in langchain?\", \"ideal\": \"Vector stores implement the VectorStore interface, which exposes a way to search using similarity search (based on embedding or vector similarity). Doc stores implement the DocStore interface, which allows a search() method to return documents, but this needn't use vector similarity.\"}\n",
      " \n",
      "\n",
      "{\"input\": \"Will this work?\\n\\n```\\nfrom langchain.chat_models import ChatOpenAI\\n\\nllm = ChatOpenAI(model=\\\"claude-2\\\")\\nllm.predict(\\\"Hi\\\")\\n```\", \"ideal\": \"No. Claude-2 is a closed-source model developed by Anthropic. The ChatOpenAI class interfaces with OpenAI's chat models.\"}\n",
      " \n",
      "\n",
      "{\"input\": \"what's the strict argument in the json output function parser mean?\", \"ideal\": \"The strict argument in the JSON output function parser determines whether non-JSON-compliant strings are allowed during parsing.\"}\n",
      " \n",
      "\n",
      "{\"input\": \"what's stuff mean in chain_type=stuff\", \"ideal\": \"\\\"stuff\\\" refers to the StuffDocumentsCHain. In this case \\\"stuff\\\" means \\\"to stuff\\\" or \\\"to fill\\\"). It takes a list of documents, inserts them all into a prompt and passes that prompt to an LLM.\"}\n",
      " \n",
      "\n",
      "{\"input\": \"Is stream natively supported by the vertex ai llm?\", \"ideal\": \"yes\"}\n",
      " \n",
      "\n",
      "{\"input\": \"Is stream natively supported by Petals llm\", \"ideal\": \"no\"}\n",
      " \n",
      "\n",
      "{\"input\": \"whats the difference between run house and click house\", \"ideal\": \"ClickHouse is an open source database for real-time apps and analytics. Runhouse allows remote compute and data across environments and users. \"}\n",
      " \n",
      "\n",
      "{\"input\": \"is gpt-3.5-turbo an lstm?\", \"ideal\": \"no\"}\n",
      " \n",
      "\n",
      "{\"input\": \"how do i initialize OpenAIAnthropicVectorStore?\", \"ideal\": \"I'm not sure what OpenAIAnthropicVectorStore is, though there is a similar-sounding class OpenSearchVectorSearch.\"} \n",
      "\n",
      "[{'input': 'in code, how can i add a system message at the end of the conversation history to influence the output of the llm', 'ideal': '\\n```python\\nfrom langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\\n\\nprompt_template = ChatPromptTemplate.from_messages(\\n    [\\n        (\"human\", \"{user_input}\"),\\n        MessagesPlaceholder(variable_name=\"chat_history\")\\n        (\"system\", \"New System Instruction here\"),\\n    ]\\n)\\n```\\nWhen formatted, it will generate 1 human/user message and 1 system message, as well as messages for the whole conversation history.\\n```python\\nprompt_template.format_messages(user_input=\"Hello\", chat_history=[])\\n```\\n[HumanMessage(content=\\'Hello\\', additional_kwargs={}, example=False),\\n SystemMessage(content=\\'New System Instruction here\\', additional_kwargs={})]\\n'}, {'input': 'how to use enums in Tools that will be used by functions agent?', 'ideal': \"If you're defining a dynamic tool (either using the @tool decorator or the `.from_function(...) classmethod), simply using the enum typehint is enough. If you are manually specifying an `args_schema` as a pydantic basemodel, you can specify the enum as the argument typehint there.\"}, {'input': 'How do I deal with openai rate limiting by having a backup model? Show me with code.', 'ideal': 'A straightforward way to do this is to specify fallback models using LangChain Expression Language. Example below:\\n```python     \\nfrom langchain.chat_models import ChatOpenAI\\n\\nbad_llm = ChatOpenAI(model_name=\"gpt-fake\")\\ngood_llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\")\\nllm = bad_llm.with_fallbacks([good_llm])\\nllm.invoke(\"Why did the the chicken cross the road?\")\\n```'}, {'input': \"I'm runing my own model using vllm. How do I connect it to LangChain?\", 'ideal': 'LangChain has a VLLM integration:\"\\n```python\\nfrom langchain.llms import VLLM\\n\\nllm = VLLM(model=\"mosaicml/mpt-7b\",\\n           trust_remote_code=True,  # mandatory for hf models\\n           max_new_tokens=128,\\n           top_k=10,\\n           top_p=0.95,\\n           temperature=0.8,\\n)\\n\\nprint(llm(\"What is the capital of France ?\"))'}, {'input': \"What's a runnable lambda?\", 'ideal': \"A runnable lambda is an object that implements LangChain's `Runnable` interface and runs a callbale (i.e., a function). Note the function must accept a single argument.\"}, {'input': \"What's an LLMChain\", 'ideal': 'An LLMChain is a chain that composes basic LLM functionality. It consists of a PromptTemplate and a language model (either an LLM or chat model). It formats the prompt template using the input key values provided (and also memory key values, if available), passes the formatted string to LLM and returns the LLM output.'}, {'input': \"What's the difference between a prompt template and a chat prompt template?\", 'ideal': \"A prompt template wraps instructions, examples, and other formatting information for the llm's prompts. A chat prompt template provides similar functionality for chat models. The largest difference is that chat prompt templates expose a `format_messages` method to convert the  prompt template into a list of structured chat messages.\"}, {'input': 'What is ConversationSummaryBufferMemory?', 'ideal': 'ConversationSummaryBufferMemory is a memory type that combines the ideas of keeping a buffer of recent interactions and compiling them into a summary. It uses token length to determine when to flush interactions.'}, {'input': 'What does LangChain offer for token counting?', 'ideal': 'LangChain offers a context manager that allows you to count tokens, as well as integrations with services like LangSmith that can help track tokens.'}, {'input': 'What is the purpose of caching embeddings?', 'ideal': 'Caching embeddings allows for storing or temporarily caching embeddings to avoid the need for recomputing them.'}, {'input': 'What is the purpose of the Retry parser?', 'ideal': 'The Retry parser is used when the LLM output is not in the expected format. It passes the input and initial output back to the LLM so it can try again with the correct format.'}, {'input': 'What does ConversationBufferWindowMemory do?', 'ideal': 'ConversationBufferWindowMemory keeps a list of the interactions of the conversation over time. It only uses the last K interactions. This can be useful for keeping a sliding window of the most recent interactions, so the buffer does not get too large.'}, {'input': 'What is the main purpose of the time-weighted vector store retriever?', 'ideal': 'The main purpose of the time-weighted vector store retriever is to use a combination of semantic similarity and a time decay to retrieve relevant vectors.'}, {'input': 'What serialization format is used to serialize chains to and from disk?', 'ideal': 'The serialization format used is JSON or YAML.'}, {'input': \"whats an agent type that works well with Anthropic's models and supports structured output?\", 'ideal': \"The XML agent works well with Anthropic's Claude 2 model. Here's some code for doing so:\\n\\n```python\\nfrom langchain.agents import XMLAgent, tool, AgentExecutor\\nfrom langchain.chat_models import ChatAnthropic\\nfrom langchain.chains import LLMChain\\nmodel = ChatAnthropic(model='claude-2')\\nchain = LLMChain(\\nllm=model,\\nprompt=XMLAgent.get_default_prompt(),\\noutput_parser=XMLAgent.get_default_output_parser()\\n)\\nagent = XMLAgent(tools=tool_list, llm_chain=chain)\\nagent_executor = AgentExecutor(agent=agent, tools=tool_list, verbose=True)\\n```\"}, {'input': 'I want to save the configuration for a given LLM. Show me how to do that.', 'ideal': 'To save the configuration for a given LLM, you can follow these steps:\\n\\nFirst, let\\'s go over loading an LLM configuration from disk. LLM configurations can be saved in two formats: JSON or YAML, but they are loaded in the same way.\\n\\nFor a JSON-formatted LLM configuration (llm.json):\\n\\n```json\\n{\\n    \"model_name\": \"text-davinci-003\",\\n    \"temperature\": 0.7,\\n    \"max_tokens\": 256,\\n    \"top_p\": 1.0,\\n    \"frequency_penalty\": 0.0,\\n    \"presence_penalty\": 0.0,\\n    \"n\": 1,\\n    \"best_of\": 1,\\n    \"request_timeout\": null,\\n    \"_type\": \"openai\"\\n}\\n```\\n\\nLoad the configuration like this:\\n\\n```python\\nfrom langchain.llms.loading import load_llm\\n\\nllm = load_llm(\"llm.json\")\\n```\\n\\nFor a YAML-formatted LLM configuration (llm.yaml):\\n\\n```yaml\\n_type: openai\\nbest_of: 1\\nfrequency_penalty: 0.0\\nmax_tokens: 256\\nmodel_name: text-davinci-003\\nn: 1\\npresence_penalty: 0.0\\nrequest_timeout: null\\ntemperature: 0.7\\ntop_p: 1.0\\n```\\n\\nLoad the configuration like this:\\n\\n```python\\nfrom langchain.llms.loading import load_llm\\n\\nllm = load_llm(\"llm.yaml\")\\n```\\n\\nTo save an LLM configuration from memory to a serialized version, you can use the `.save` method. This method supports both JSON and YAML formats:\\n\\n```python\\nllm.save(\"llm.json\")\\nllm.save(\"llm.yaml\")\\n```\\n\\nThis way, you can easily save and load LLM configurations to and from disk.'}, {'input': 'Show me an example using Weaviate, but customizing the VectorStoreRetriever to return the top 10 k nearest neighbors. ', 'ideal': 'To customize the Weaviate client and return the top 10 k nearest neighbors, you can utilize the `as_retriever` method with the appropriate parameters. Here\\'s how you can achieve this:\\n\\n```python\\n# Assuming you have imported the necessary modules and classes\\n\\n# Create the Weaviate client\\nclient = weaviate.Client(url=os.environ[\"WEAVIATE_URL\"], ...)\\n\\n# Initialize the Weaviate wrapper\\nweaviate = Weaviate(client, index_name, text_key)\\n\\n# Customize the client to return top 10 k nearest neighbors using as_retriever\\ncustom_retriever = weaviate.as_retriever(\\n    search_type=\"similarity\",\\n    search_kwargs={\\n        \\'k\\': 10  # Customize the value of k as needed\\n    }\\n)\\n\\n# Now you can use the custom_retriever to perform searches\\nresults = custom_retriever.search(query, ...)\\n```'}, {'input': 'What is BabyAGI', 'ideal': 'BabyAGI is an example of an \"autonomous AI agent\" that can generate and simulate the execution of tasks based on a given objective.'}, {'input': 'What is the difference between ChatPromptTemplate and PromptTemplate?', 'ideal': 'ChatPromptTemplate and PromptTemplate are similar in their functions. PromptTemplate takes a prompt as a string that can contain variables. ChatPromptTemplate separates the prompt into structured messages of different kinds. Each chat message is associated with content, and an additional parameter called role. Some example roles include system, human (or user), and AI (or assistant).'}, {'input': 'Show me how to use RecursiveURLLoader', 'ideal': 'The RecursiveURLLoader comes from the langchain.document_loaders.recursive_url_loader module. Here\\'s an example of how to use it:\\n\\n```python\\nfrom langchain.document_loaders.recursive_url_loader import RecursiveUrlLoader\\n\\n# Create an instance of RecursiveUrlLoader with the URL you want to load\\nloader = RecursiveUrlLoader(url=\"https://example.com\")\\n\\n# Load all child links from the URL page\\ndocuments = loader.load()\\n\\n# Fetch the documents\\nfor doc in documents:\\n    print(doc)\\n```\\n\\nMake sure to replace \"https://example.com\" with the actual URL you want to load.'}, {'input': 'what are the main methods supported by Runnables', 'ideal': 'The `runnable` interface in LangChain supports several main methods for interacting with components:\\n\\n1. `stream`: This method streams back response chunks in real-time, useful for processing large computations or text generation as they become available.\\n\\n2. `invoke`: Calls the runnable chain on a single input, providing a single response output.\\n\\n3. `batch`: Calls the runnable chain on a list of inputs, returning a list of corresponding responses.\\n\\n4. `astream`: Asynchronously streams response chunks back, suitable for handling concurrent tasks.\\n\\n5. `ainvoke`: Asynchronously calls the runnable chain on a single input and returns the response.\\n\\n6. `abatch`: Asynchronously calls the runnable chain on a list of inputs, returning a list of corresponding responses.'}, {'input': 'What is html2texttransformer? Does it omit urls?', 'ideal': \"The `Html2TextTransformer` is a class provided by LangChain's `document_transformers` module. It is used to transform HTML content into clean, easy-to-read plain ASCII text.\\n\\nThis transformer is designed to convert HTML pages into human-readable text, making it suitable for scenarios where the goal is to extract content without the need to manipulate specific HTML elements.\\n\\nIf urls are present in the source Document's metadata, then they will be retained. If they are stored in the page_content itself, the class makes no guarantees that they will be retained.\"}, {'input': 'I want to return the source documents of my Weaviate retriever. Show me how', 'ideal': \"To retrieve the source documents from your Weaviate vectorstore, you need to configure your client to include the 'sources' attribute. Here's an example of how you can achieve this:\\n\\n```python\\nweaviate_client = Weaviate(\\n    client=client,\\n    index_name='index_name',\\n    text_key='text',\\n    embedding=OpenAIEmbeddings(),\\n    by_text=False,\\n    attributes=['source'],\\n)\\n```\\n\\nBy including the 'source' attribute in the attributes list, you ensure that the source documents are returned when you interact with the Weaviate client.\"}, {'input': 'what is RAG', 'ideal': 'RAG stands for Retrieval Augmented Generation, which refers to the application design of using an LLM to generate responses to user inputs grounded on retrieved information. The retrieved information can come from anywhere, though it typically is fetched from 1 or more databases, search engines, knowledge graphs, or other sources.'}, {'input': 'whats the code to load text file into a vector store', 'ideal': 'To load a text file into a vector store using LangChain\\'s document loaders, you can use the following code snippet:\\n\\n```python\\nfrom langchain.document_loaders import TextLoader\\n\\nloader = TextLoader(\"./index.txt\")\\ndocs = loader.load()\\nprint(docs)\\n```\\n\\nThis code imports the `TextLoader` class from the document_loaders module, creates an instance of the loader with the path to the text file as an argument, and then uses the `load()` method to read the file\\'s content and create a Document. The resulting Document will contain the text content of the file.'}, {'input': \"Help me debug: TypeError: Pinecone.similarity_search_with_score() got an unexpected keyword argument 'min_score'\", 'ideal': \"Your error is likely due to an incorrect parameter being passed to the 'similarity_search_with_score' function in the Pinecone vector store. The correct function signature does not include a 'min_score' argument. This function retrieves documents most similar to a given query text while providing both the documents and their corresponding similarity scores. To use 'similarity_search_with_score', ensure you provide the right arguments: 'query' for the text you want to find similar documents for, 'k' for the number of documents to retrieve (default: 4), 'filter' as an optional metadata-based filtering dictionary, and 'namespace' to specify the search namespace (default is the empty namespace ''). The function returns a list of tuples, each containing a Document and a similarity score (float). To resolve the 'TypeError' issue, ensure proper argument usage as per the documentation.  \"}, {'input': 'How can I create a simple chat model using my locally saved huggingface model', 'ideal': 'Certainly! To create a simple chat model using your locally saved Hugging Face model, follow these steps:\\n\\n1. Install the necessary Python libraries:\\n\\n- Install \\'transformers\\' library for working with models and tokenizers using \\'pip install transformers\\'.\\n\\n2. Import the `HuggingFacePipeline` class to work with your local pipeline\\n\\n```\\nfrom langchain.llms import HuggingFacePipeline\\n\\n# Create an instance of the wrapper\\nllm = HuggingFacePipeline.from_model_id(\"path/to/model_directory\", task=\"text-generation\")\\n```\\nAlternatively, load your pipeline directly:\\n```\\nfrom langchain.llms.huggingface_pipeline import HuggingFacePipeline\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\\n\\nmodel_path = \"path/to/model_dir\"\\ntokenizer = AutoTokenizer.from_pretrained(model_path)\\nmodel = AutoModelForCausalLM.from_pretrained(model_path)\\npipe = pipeline(\\n    \"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=10\\n)\\nllm = HuggingFacePipeline(pipeline=pipe)\\n```\\n\\n Finally, interact with the chat model\\n```\\nresponse = llm.invoke(\\'User: Hello, how can I help?\\')\\n```'}, {'input': 'How do I use a React Agent with an Anthropic model?', 'ideal': 'In order to use a ReAct Agent with an Anthropic model, you should first install the reuqired packages:\\n\\npip install anthropic\\n\\nNext, import the necessary classes and functions:\\n\\n    from langchain.agents import load_tools\\n    from langchain.agents import initialize_agent\\n    from langchain.agents import AgentType\\n    from langchain.chat_models import ChatAnthropic\\n\\nNext, create an instance of ChatAnthropic. Make sure you have set your API key in an environment variable called ANTHROPIC_API_KEY or pass anthropic_api_key as a parameter.\\n\\n    chat = ChatAnthropic()\\n\\nNext, load some tools for your agent to use:\\n\\n    # Specify tools (these can be any custom tool, the following are examples)\\ntools = load_tools([\"serpapi\", \"llm-math\"], llm=chat)\\n\\nFinally, let\\'s initialize an agent with the tools, the language model, and the type of agent we want to use.\\n\\n    agent = initialize_agent(tools, chat, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\\n\\nNow, enter a prompt to test it out:\\n\\n    agent.invoke(\"Who is Leo DiCaprio\\'s girlfriend? What is her current age raised to the 0.43 power?\")'}, {'input': 'How do I use Qdrant as a vector store in the conversational retrieval chain?', 'ideal': 'In order to use Qdrant as a vector store in the conversational retrieval chain, you should start by installing Qdrant:\\n\\n    pip install qdrant-client\\n\\nNext, import the necessary classes and functions:\\n\\n    from langchain.vectorstores import Qdrant\\n    from langchain.embeddings.openai import OpenAIEmbeddings\\n    from langchain.text_splitter import CharacterTextSplitter\\n    from langchain.llms import OpenAI\\n    from langchain.chains import ConversationalRetrievalChain\\n    from langchain.document_loaders import TextLoader\\n    from langchain.memory import ConversationBufferMemory\\n\\nLoad, split, and embed your documents:\\n\\n    loader = TextLoader(\"state_of_the_union.txt\")\\n    documents = loader.load()\\n\\n    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0, separator=\"\\n\")\\n    documents = text_splitter.split_documents(documents)\\n    embeddings = OpenAIEmbeddings()\\n\\nNext, connect to Qdrant. This is for storing locally on-disk, but you can also use an on-premise server deployment or store on the Qdrant cloud:\\n\\n    qdrant = Qdrant.from_documents(\\n        documents,\\n        embeddings,\\n        path=\"/tmp/local_qdrant\",\\n        collection_name=\"my_documents\",\\n    )\\n\\nNext, create a memory object, which is necessary to track the inputs/outputs and hold a conversation:\\n\\n    memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\\n\\nNow initialize the ConversationalRetrievalChain:\\n\\n    qa = ConversationalRetrievalChain.from_llm(OpenAI(temperature=0), qdrant.as_retriever(), memory=memory)\\n\\nNow, you can create queries to test it out:\\n\\n    query = \"What did the president say about Ketanji Brown Jackson\"\\n    result = qa({\"question\": query})\\n\\nOutput:\\n\\n    The president said that he had nominated Ketanji Brown Jackson to serve on the United States Supreme Court four days ago.\\n\\n    query = \"Who are they succeeding?\"\\n    result = qa({\"question\": query})\\n\\nOutput:\\n\\n    Justice Breyer.'}, {'input': 'How do I use Summary Memory in the conversational retrieval chain?', 'ideal': 'In order to use Summary Memory in the conversational retrieval chain, you should first import the necessary classes and functions:\\n\\n        from langchain.memory import ConversationSummaryMemory\\n    from langchain.embeddings.openai import OpenAIEmbeddings\\n    from langchain.vectorstores import Chroma # You can choose any LangChain vectorstore\\n    from langchain.text_splitter import CharacterTextSplitter\\n    from langchain.chat_models import ChatOpenAI # You can choose any LangChain vectorstore\\n    from langchain.chains import ConversationalRetrievalChain\\n    from langchain.document_loaders import TextLoader\\n\\nSplit and embed your documents, and add them to your selected vectorstore:\\n\\n    loader = TextLoader(\"state_of_the_union.txt\")\\n    documents = loader.load()\\n\\n    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0, separator=\"\\n\")\\n    documents = text_splitter.split_documents(documents)\\n    embeddings = OpenAIEmbeddings()\\n    vectorstore = Chroma.from_documents(documents, embeddings)\\n\\nNext, create a ConversationSummaryMemory object:\\n\\n    memory = ConversationSummaryMemory(llm=ChatOpenAI(model=\"gpt-3.5-turbo\"), memory_key=\"chat_history\", return_messages=True)\\n\\nNow initialize the ConversationalRetrievalChain:\\n\\n    qa = ConversationalRetrievalChain.from_llm(ChatOpenAI(model=\"gpt-3.5-turbo\",temperature=0), vectorstore.as_retriever(), memory=memory)\\n\\nNow, you can create queries to test it out:\\n\\n    query = \"What did the president say about Ketanji Brown Jackson\"\\n    result = qa({\"question\": query})\\n\\nOutput:\\n\\n    The president said that he had nominated Ketanji Brown Jackson to serve on the United States Supreme Court four days ago.\\n\\n    query = \"Who are they succeeding?\"\\n    result = qa.invoke({\"question\": query})\\n\\nOutput:\\n\\n    Justice Breyer.'}, {'input': 'How do I load Youtube transcripts and CSV documents?', 'ideal': 'To load YouTube transcripts in LangChain, you can use the YoutubeLoader document loader. Here\\'s an example of how to do it:\\n\\nInstall the required packages:\\n```\\npip install langchain youtube_transcript_api pytube\\n```\\n\\nImport the necessary classes:\\n\\n\\n    from langchain.document_loaders import YoutubeLoader\\n\\n\\nCreate an instance of the YoutubeLoader class and provide the YouTube video URL:\\n\\n\\n    loader = YoutubeLoader.from_youtube_url(\"https://www.youtube.com/watch?v=QsYGlZkevEg\", add_video_info=True)\\n\\n\\nLoad the transcript using the load method:\\n\\n    video = loader.load()\\n\\nTo load CSV documents you can use CSVLoader. Follow the steps below:\\n\\nImport the necessary classes:\\n\\n    from langchain.document_loaders import CSVLoader\\n\\n\\nCreate an instance of the CSVLoader class and provide the path to the CSV file:\\n\\n\\n    loader = CSVLoader(file_path=\"./example_data/mlb_teams_2012.csv\")\\n\\nLoad the CSV document using the load method:\\n\\n    loader.load()'}, {'input': 'my agent keeps getting an OutputParserException is something i can set to make it take care of these?', 'ideal': \"You can control the agent's behavior for output parsing errors by initializing the agent with `handle_parsing_errors`=True. Then the agent will see 'Handle errors with Invalid or incomplete response'. You can also set a string to provide a custom error message to the agent, or provide a custom error handling function.\"}, {'input': 'What does ReAct mean?', 'ideal': \"ReAct in LangChain refers to a type of agent that 'Reasons' and 'Act's in a loop.\"}, {'input': 'What are intermediate steps in langchain?', 'ideal': 'Intermediate stps represent previous agent actions and corresponding outputs. They are used to help the agent know what work has already been done in pursuit of a goal.'}, {'input': 'im getting a rate limit error for my llm - how do I have it automatically go to a different model?', 'ideal': \"To switch to a different model when an LLM is gettin rate limited, you can use 'fallbacks'. For instance, if you have too llms: openai_llm and anthropic_llm, you can have the program fall back to the anthropic llm by calling `llm = openai_llm.with_fallbacks([anthropic_llm])`\"}, {'input': \"What's function calling\", 'ideal': 'Function calling refers to the ability to define functions for LLMs to let them choose to call with structured arguments.'}, {'input': 'what is the langserve', 'ideal': 'LangServe is an easy way to deploy any LangChain runnable, chain, or agent. It uses FastAPI and pydantic to automatically create an API server from any compatible LangChain component.'}, {'input': \"What's a string evaluator\", 'ideal': \"A string evaluator in LangChain is an object that assesses the performance of a language model's predicted response by comparing it to an input string and/or reference string.\"}, {'input': 'whats a toolkit', 'ideal': 'Toolkits are collections of tools that are designed to be used together for specific tasks and have convenience loading methods.'}, {'input': 'is langchain compatible with pydantic v2?', 'ideal': 'As of langchain>=0.0.267, LangChain will allow users to install either Pydantic V1 or V2, though it internally will use pydantic V1.'}, {'input': 'How can I parallelize calls in LangChain?', 'ideal': \"To make parallel calls from a LangChain object, use the 'batch()' (or asynchronous 'abatch()') method. You can also use a `RunnableParallel` object.\"}, {'input': 'how many llm api calls are made in OpenAIFunctionsAgent', 'ideal': \"The OpenAIFunctionsAgent is an agent driven by OpenAI's function-powered API. It uses the function_call parameter from OpenAI to determine which tools to use for each API call. In general, the number of API calls made by the OpenAIFunctionsAgent depends on the question and the number of tools used in the agent's pipeline. At minimum, one call will be made to the LLM to (optionally) select a tool or directly respond. For more complex questions or intents, the agent may make many calls.\"}, {'input': 'What are some ways of doing retrieval augmented generation?', 'ideal': 'Retrieval Augmented Generation (RAG) is a process that involves retrieving external data and using it in the generation step of language models. LangChain provides various building blocks for RAG applications. Here are some ways of doing retrieval augmented generation:\\n\\n- Document loaders: LangChain offers over 100 document loaders to load documents from different sources, including private S3 buckets and public websites.\\n- Document transformers: These help in fetching relevant parts of documents by splitting or chunking large documents into smaller chunks.\\n- Text embedding models: LangChain integrates with over 25 different embedding providers and methods to create embeddings for documents, allowing efficient similarity-based retrieval.\\n- Vector stores: With the rise of embeddings, LangChain supports over 50 vector stores for efficient storage and searching of embeddings.\\n- Retrievers: LangChain supports various retrieval algorithms, including semantic search, parent document retriever, self-query retriever, ensemble retriever, and more, to retrieve the data from the database.\\n\\nThese components work together to enable retrieval augmented generation in LangChain. [0] [1]'}, {'input': 'whats the difference between a handler and an inheritable_handler?', 'ideal': 'In Langchain, both `handler` and `inheritable_handler` are types of callback handlers. Handlers can refer to inheritable handlers or local handlers. Inheritable handlers are propagated to other objects that are called within a Runnable, chain, tool, or other object. Local handlers are only called by the object they are assigned to and not called by any other objects.'}, {'input': 'What is a chain?', 'ideal': 'A \\'Chain\\' is a class in LangChain that composes a number of components together into a single object. An example would be the LLMChain, which formats an input as a prompt, calls an LLM, and then parses the output. \"chaining\" also can generically refer to composing llms, functions, and other operations together into a larger program.'}, {'input': 'What is LangChain Expression Language?', 'ideal': 'LangChain Expression Language (LCEL) is a declarative way to easily compose chains together. It provides several benefits for building applications:\\n\\n- Async, Batch, and Streaming Support: LCEL chains have full sync, async, batch, and streaming support, making it easy to prototype in a Jupyter notebook and then expose it as an async streaming interface.\\n- Fallbacks: LCEL allows you to easily attach fallbacks to any chain, handling errors gracefully.\\n- Parallelism: LCEL syntax automatically runs components that can be run in parallel.\\n- Seamless LangSmith Tracing Integration: LCEL logs all steps to LangSmith for observability and debuggability. [0]\\n\\nLCEL provides an intuitive and readable syntax for composition and supports streaming, async calls, batching, parallelization, retries, and more. It allows you to combine different components, such as prompts, models, and output parsers, to build powerful applications. LCEL chains can be called synchronously or asynchronously, and they support intermediate result access, input/output schemas, and tracing with LangSmith. [3]'}, {'input': 'which document laaoder should i use for a loading a single web apage?', 'ideal': 'For a single web page, the WebBaseLoader or AsyncHtmlLoader should suffice. If you want to load multiple pages by traversing a whole website, you can use the SitemapLoader or RecursiveUrlLoader.'}, {'input': 'whats the difference between run_id and example_id', 'ideal': 'The `run_id` and `example_id` are both identifiers used in LangChain and LangSmith, but they serve different purposes. The `run_id` is a unique ID assigned to each individual \"run\" in a trace, which represents a unit of work whenever a runnable object such as an LLM or chain is called. An `example_id` is a unique ID assigned to each row (or \"example\") in a LangSmith dataset.'}, {'input': 'What is an agent', 'ideal': 'An agent is a component in the Langchain framework that uses a language model (LLM) to determine which actions to take and in what order. It is responsible for making decisions and interacting with tools to accomplish specific objectives.'}, {'input': 'how do I search and filter metadata in redis vectorstore?', 'ideal': 'To perform a similarity search with metadata filtering in LangChain using the Redis vectorstore, you can use Redis filter expressions. The following filter operations are available: RedisText, RedisNum, and RedisTag. Use these when calling `similarity_search()`.'}, {'input': 'how do I control the maximum number requests that can be made at the same time when making batch calls?', 'ideal': \"To limit the maximum number of concurrent requests for batch calls in LangChain, you can call the `.batch()` method and configure the 'max_concurrency' paramater within the runnable config.  For example, the following code calls the chain with a maximum of 2 concurrent calls:\\n```python\\nchain.batch([{'topic': 'bears'}, {'topic': 'cats'}, {'topic': 'dogs'}], config={'max_concurrency': 2})\\n```\"}, {'input': 'can i cache LLM calls in sqlite?', 'ideal': 'To add LLM caching in sqlite, you can use the SQLiteCache class. For example:\\n```python\\nfrom langchain.globals import set_llm_cache\\nfrom langchain.cache import SQLiteCache\\nfrom langchain.chat_models import ChatOpenAI\\n\\nllm = ChatOpenAI()\\n\\nset_llm_cache(SQLiteCache(database_path=\".langchain.db\"))\\nllm.invoke(\"Tell me a joke\")\\n```'}, {'input': 'how can I create a vectorstore from the texts in the list', 'ideal': 'Each LangChain vectorstore class implements a \"from_texts\" class method that accepts a list of string texts, an instance of an `Embeddings` object to embed those texts, and an optional list of metadata dictionaries to add to the resulting documents.'}, {'input': 'what does on_tool_start mean?', 'ideal': 'The on_tool_start method is defined classes that implement LangChain\\'s BaseCallbackHandler interface. When a \"Tool\" object is invoked, and callbacks are provided, the \\'on_tool_start\\' method is called prior to executing the tool itself.'}, {'input': 'what does this do? return RunnableBranch(\\n        (\\n            RunnableLambda(lambda x: bool(x.get(\"chat_history\"))).with_config(\\n                run_name=\"HasChatHistoryCheck\"\\n            ),\\n            conversation_chain.with_config(run_name=\"RetrievalChainWithHistory\"),\\n        ),\\n        (\\n            RunnableLambda(itemgetter(\"question\")).with_config(\\n                run_name=\"Itemgetter:question\"\\n            )\\n            | retriever\\n        ).with_config(run_name=\"RetrievalChainWithNoHistory\"),\\n    ).with_config(run_name=\"RouteDependingOnChatHistory\")', 'ideal': 'This code defines a runnable chain with two branches. The first branch checks if the input has a \\'chat_history\\' key and runs the conversation_chain if it does. Otherwise uses python stdlib\\'s \"itemgetter\" to get the \"question\" item from the input dictionary. Each \"with_config\" call assigns a friendly \"run_name\" to that step in the traced DAG.'}, {'input': 'How can I use OpenAI functions to get structured outputs in a chain?', 'ideal': 'You can use OpenAI functions to get structured outputs in a chain by using the create_structured_output_runnable function. This function takes the desired structured output either as a Pydantic class or as JsonSchema. If you pass in a model explicitly, make sure it supports the OpenAI function-calling API. You can then invoke the runnable with the input data, and it will return the structured output. You can also use the bind_functions() method on the ChatOpenAI class to do this directly from an instance of ChatOpenAI.'}, {'input': 'hw do i create a prompt template for my chat bot', 'ideal': 'To create a prompt template for a chat bot, you can use the `ChatPromptTemplate` class. You can initialize it with a list of chat messages, where each message has a role and content. The role can be \\'system\\', \\'human\\', or \\'ai\\'. For example:\\n\\n```python\\nfrom langchain.prompts import ChatPromptTemplate\\n\\nchat_template = ChatPromptTemplate.from_messages(\\n    [\\n        (\\'system\\', \\'You are a helpful AI bot. Your name is {name}.\\'),\\n        (\\'human\\', \\'Hello, how are you doing?\\'),\\n        (\\'ai\\', \\'I\\'m doing well, thanks!\\'),\\n        (\\'human\\', \\'{user_input}\\'),\\n    ]\\n)\\n\\nmessages = chat_template.format_messages(name=\\'Bob\\', user_input=\\'What is your name?\\')\\n```\\n\\nYou can also use different subclasses of the `ChatMessagePromptTemplate` class, such as `SystemMessagePromptTemplate` and `HumanMessagePromptTemplate`, to customize the role or content of specific messages in the template. Finally, you can use the \"MessagesPlaceholder\" class to format chat history as a sequence of messages.'}, {'input': 'what method should subclasses override if they can start producing output while input is still being generated', 'ideal': 'Subclasses should override the transform() method if they can start producing output while input is still being generated.'}, {'input': 'what does runnable mean', 'ideal': 'In the context of LangChain, runnable\\'s are the building blocks of the LangChain Expression Language. They implement the \"Runnable\" base class and represent a unit of work that can be invoked, batched, streamed, transformed and composed.'}, {'input': 'I am summarizing text contained in the variable chunks with load_summarize_chain.\\n\\nchain = load_summarize_chain(llm, chain_type=\"map_reduce\")\\nchain.run(chunks)\\nI would like to add a tag when I run the chain that langsmith will capture. How?', 'ideal': 'all chains inherit from the runnable interface. So they support an invoke method. The invoke method takes the input and a config.\\nFor the config you can pass in { \\'tags\\': [ \\'tag1\\' ] }.\\n\\nchain.invoke( input, config={ \\'tags\\': [\\'tags1\\'] }.\\n\\nIf you want to just tag that run (and not propagate the tags to child calls), you can pass the tags in as a list of strings to the call to `load_summarize_chain(llm, chain_type=\"map_reduce\", tags=[\\'tag1\\'])`.\\nFor older methods in the LangChain API like run, __call__, etc. you can also add tags directly when calling, such as with `chain.run(\"foo\", tags=[\\'tag1])`.'}, {'input': 'Let\\'s say I have a chain like:\\n\\nmodel_call_1 = (\\n  RunnablePassthrough()\\n  | prompt\\n  | model_parser\\n)\\n# ...\\nchain = model_call_1 #| { \"attr\": model_call_2 } | model_call_3\\nHow can I print out the filled out prompts for each model call?', 'ideal': 'There are a few options. You can use set_debug(True) to print out each run to the console:\\n```\\nfrom langchain.globals import set_debug\\n\\nset_debug(True)\\n```\\nThis uses the ConsoleCallbackHandler. You can also trace with LangSmith by configuring your environment:\\n\\n```\\nexport LANGCHAIN_API_KEY=\"Your API Key\"\\nexport LANGCHAIN_TRACING_V2=true\\n```'}, {'input': 'soooo, is it possible to pass any kwargs for similarity thresholds or K-top documents in the multivectorretriever as we do in normal retrievers?', 'ideal': 'You can assign search_kwargs (such as k) to the instance of the MultiVectorRetriever'}, {'input': 'whats the stuff documents chain?', 'ideal': 'The stuff documents chain (\"stuff\" as in \"to stuff\" or \"to fill\") is the most straightforward of the document chains. It takes a list of documents, inserts them all into a prompt and passes that prompt to an LLM.\\n\\nThis chain is well-suited for applications where documents are small and only a few are passed in for most calls.'}, {'input': 'Does AlephAlpha support token-level streaming natively in LangChain?', 'ideal': \"No, while the stream() method is provided in all langchain runnables, AlephAlpha doesn't natively implement streaming on the token level.\"}, {'input': 'How to use a prompt template with chat history', 'ideal': 'To add chat history to a prompt template, you can use the MessagesPlaceholder class.\\n\\n```\\nfrom langchain.prompts import MessagesPlaceholder, ChatPromptTemplate\\n\\nchat_template = ChatPromptTemplate.from_messages(\\n    [\\n        (\"system\", \"You are a helpful AI bot.\"),\\n        MessagesPlaceholder(variable_name=\"chat_history\")\\n        (\"human\", \"{input}\"),\\n    ]\\n)\\n```\\n'}, {'input': 'How do i run llama 2 in langchain', 'ideal': 'Llama 2 can be run through a number of LangChain llm and chat model integrations. A non-exhaustive list includes LlamaCPP, Ollama, Huggingface, Fireworks, and Replicate, HuggingFace. These imports would be\\n```python\\nfrom langchain.llms import Fireworks, HuggingFacePipeline, LlamaCpp, Ollama, Replicate, HuggingFaceHub, HuggingFaceTextGenInference, HuggingFaceEndpoint\\nfrom langchain.chat_models import ChatFireworks, ChatOllama\\n```\\n\\nInitializing any of these classes with a parameter pointing to the llama2 model should suffice. Noteably, there is no LangChain class called Llama2, Llama, or Huggingface (with no suffixes). The classes listed above are also case-sensitive.'}, {'input': \"What's the difference between the OpenAI and ChatOpenAI classes?\", 'ideal': \"OpenAI wraps OpenAI's completions endpoint, whereas the ChatOpenAI class wraps the chat endpoint. Both interfadce with LLM's, but the interface chat models is structured as chat messages, which contain information about the role of speaker. Examples of roles include system, human/user, and AI/assistant.\"}, {'input': 'how do i run llama on vllm', 'ideal': 'You can run HuggingFace models in vLLM, so you can select the model name of a compatible Llama model then use the VLLM class in LangChain:\\n ```python\\nfrom langchain.llms import VLLM\\n\\nllm = VLLM(\\n    model=\"name-of-llama-model\",\\n    trust_remote_code=True,  # mandatory for hf models\\n)\\n\\nprint(llm(\"What is the capital of France ?\"))\\n```'}, {'input': 'What class type is returned by initialize_agent', 'ideal': 'AgentExecutor'}, {'input': 'what does runnable.predict() mean?', 'ideal': 'The runnable base class does not have a predict() method, but the LLM classes (which inherit from Runnable) do. For those classes, predict() formats a prompt with keyword args and passes it to the LLM.'}, {'input': 'how to run a runnable', 'ideal': 'Use invoke(), stream(), batch(), or their async equivalents (ainvoke, astream, and abatch)'}, {'input': 'how do I run gpt-4 on anthropic?', 'ideal': 'GPT-4 is a model trained by OpenAI and not provided by Anthropic.'}, {'input': 'who to email for filing cves', 'ideal': 'security@langchain.dev'}, {'input': 'What class is made when using | in langchain?', 'ideal': 'The | pipe operator combines (or coerces) two runnables ( or a runnable and a function or dict or other compatible primitive) into a new runnable, namely a \"RunnableSequence.\"'}, {'input': 'How do i run gpt-4 locally?', 'ideal': 'GPT-4 is a closed-source model provided through an API by OpenAI. While you can connect to GPT-4 via the API, you cannot run the model locally.'}, {'input': 'How do i run lcel in java', 'ideal': 'LangChain, along with LCEL, or LangChain Expression Language, is not natively offered in java. LangChain is available in both python and javascript/typescript.'}, {'input': \"What's the difference between a document loader and a chat loader?\", 'ideal': \"A document loader is used to load data as Document objects, which are pieces of text with associated metadata. A chat loader is specifically designed to load chat messages from different platforms, such as Discord, Facebook Messenger, Slack, and Telegram. Chat loaders load these conversations as ChatSessions, which contain a list of BaseMessage's\"}, {'input': 'whats the diff between a docstore and a vector store in langchain?', 'ideal': \"Vector stores implement the VectorStore interface, which exposes a way to search using similarity search (based on embedding or vector similarity). Doc stores implement the DocStore interface, which allows a search() method to return documents, but this needn't use vector similarity.\"}, {'input': 'Will this work?\\n\\n```\\nfrom langchain.chat_models import ChatOpenAI\\n\\nllm = ChatOpenAI(model=\"claude-2\")\\nllm.predict(\"Hi\")\\n```', 'ideal': \"No. Claude-2 is a closed-source model developed by Anthropic. The ChatOpenAI class interfaces with OpenAI's chat models.\"}, {'input': \"what's the strict argument in the json output function parser mean?\", 'ideal': 'The strict argument in the JSON output function parser determines whether non-JSON-compliant strings are allowed during parsing.'}, {'input': \"what's stuff mean in chain_type=stuff\", 'ideal': '\"stuff\" refers to the StuffDocumentsCHain. In this case \"stuff\" means \"to stuff\" or \"to fill\"). It takes a list of documents, inserts them all into a prompt and passes that prompt to an LLM.'}, {'input': 'Is stream natively supported by the vertex ai llm?', 'ideal': 'yes'}, {'input': 'Is stream natively supported by Petals llm', 'ideal': 'no'}, {'input': 'whats the difference between run house and click house', 'ideal': 'ClickHouse is an open source database for real-time apps and analytics. Runhouse allows remote compute and data across environments and users. '}, {'input': 'is gpt-3.5-turbo an lstm?', 'ideal': 'no'}, {'input': 'how do i initialize OpenAIAnthropicVectorStore?', 'ideal': \"I'm not sure what OpenAIAnthropicVectorStore is, though there is a similar-sounding class OpenSearchVectorSearch.\"}]\n",
      "8523\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "\n",
    "def read_jsonl_file(file_path):\n",
    "    size = 0\n",
    "    data = []\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            # Load each line as a JSON object\n",
    "            print(line,'\\n')\n",
    "            json_object = json.loads(line.strip())\n",
    "            data.append(json_object)\n",
    "            size += num_tokens(json_object['input'])\n",
    "            size += num_tokens(json_object['ideal'])\n",
    "            \n",
    "    return data,size \n",
    "\n",
    "# Example usage\n",
    "file_path = 'eval.jsonl'\n",
    "jsonl_data,size = read_jsonl_file(file_path)\n",
    "\n",
    "# Now, jsonl_data contains a list of dictionaries, each representing a JSON object from the file\n",
    "print(jsonl_data)\n",
    "print(size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ce950c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "```python\n",
      "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
      "\n",
      "prompt_template = ChatPromptTemplate.from_messages(\n",
      "    [\n",
      "        (\"human\", \"{user_input}\"),\n",
      "        MessagesPlaceholder(variable_name=\"chat_history\")\n",
      "        (\"system\", \"New System Instruction here\"),\n",
      "    ]\n",
      ")\n",
      "```\n",
      "When formatted, it will generate 1 human/user message and 1 system message, as well as messages for the whole conversation history.\n",
      "```python\n",
      "prompt_template.format_messages(user_input=\"Hello\", chat_history=[])\n",
      "```\n",
      "[HumanMessage(content='Hello', additional_kwargs={}, example=False),\n",
      " SystemMessage(content='New System Instruction here', additional_kwargs={})]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4761dd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai  # for generating embeddings\n",
    "import pandas as pd  # for DataFrames to store article sections and embeddings\n",
    "import re  # for cutting <ref> links out of Wikipedia articles\n",
    "import tiktoken  # for counting tokens\n",
    "\n",
    "GPT_MODEL = \"gpt-4\"  # only matters insofar as it selects which tokenizer to use\n",
    "\n",
    "\n",
    "def num_tokens(text: str, model: str = GPT_MODEL) -> int:\n",
    "    \"\"\"Return the number of tokens in a string.\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    return len(encoding.encode(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2dcb0dac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tokens(\"hello this is me manish bhatta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a101e5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = size/1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c272283",
   "metadata": {},
   "source": [
    "price per evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7b7b067b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8523"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9c2300ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25569"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans* 0.03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dc151642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1250.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1000*5 / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "669e166d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107.5"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1250*86 / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4467bbb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.075"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "107.5 * 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3296f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "okprofessor",
   "language": "python",
   "name": "okprofessor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
